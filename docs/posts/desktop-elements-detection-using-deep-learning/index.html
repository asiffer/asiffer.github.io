<!DOCTYPE html>
<html lang="en">
  <head>
    
      <title>
        Desktop Elements Detection Using Deep Learning ::
        Alban Siffer
      </title>
    
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta
  name="description"
  content="Notes
The original post (written with Joseph Paillard) has been published on the Amossys blog. Here, some elements related to the Amossys company (and not paramount on this blog) have been removed.
  Prediction sample 
Context This project has been carried out as part of the AI &amp;amp; Cyber challenge that took place during the European Cyber Week (ECW) 2020. This year, the challenge was to emulate human behavior on a computer."
/>
<meta
  name="keywords"
  content=""
/>
<meta name="robots" content="noodp" />
<link rel="canonical" href="https://asiffer.github.io/posts/desktop-elements-detection-using-deep-learning/" />





<link rel="stylesheet" href="https://asiffer.github.io/assets/style.css" />

<link rel="stylesheet" href="https://asiffer.github.io/style.css" />


<link
  rel="apple-touch-icon-precomposed"
  sizes="144x144"
  href="https://asiffer.github.io/img/apple-touch-icon-144-precomposed.png"
/>
<link rel="shortcut icon" href="https://asiffer.github.io/img/favicon.png" />


<link href="https://asiffer.github.io/assets/fonts/Inter-Italic.woff2" rel="preload" type="font/woff2" as="font" crossorigin="">
<link href="https://asiffer.github.io/assets/fonts/Inter-Regular.woff2" rel="preload" type="font/woff2" as="font" crossorigin="">
<link href="https://asiffer.github.io/assets/fonts/Inter-Medium.woff2" rel="preload" type="font/woff2" as="font" crossorigin="">
<link href="https://asiffer.github.io/assets/fonts/Inter-MediumItalic.woff2" rel="preload" type="font/woff2" as="font" crossorigin="">
<link href="https://asiffer.github.io/assets/fonts/Inter-Bold.woff2" rel="preload" type="font/woff2" as="font" crossorigin="">
<link href="https://asiffer.github.io/assets/fonts/Inter-BoldItalic.woff2" rel="preload" type="font/woff2" as="font" crossorigin="">


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Desktop Elements Detection Using Deep Learning"/>
<meta name="twitter:description" content="We detail how we detect desktop components on screenshots. The method we present is a part of our final solution that made us win the ECW2020 challenge."/>



<meta property="og:title" content="Desktop Elements Detection Using Deep Learning" />
<meta property="og:description" content="We detail how we detect desktop components on screenshots. The method we present is a part of our final solution that made us win the ECW2020 challenge." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://asiffer.github.io/posts/desktop-elements-detection-using-deep-learning/" />
<meta property="article:published_time" content="2021-01-21T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-01-21T00:00:00+00:00" />






  </head>
  <body class="dark-theme">
    <div class="container">
      <header class="header">
  <span class="header__inner">
    <a
  href="/"
  class="logo"
  style="text-decoration: none;"
>
  
    <span class="logo__mark"><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44">
  <path fill="none" d="M15 8l14.729 14.382L15 35.367" />
</svg>
</span>
    <span class="logo__text"
      >cd /home/âˆ‡</span
    >
    <span class="logo__cursor"></span>
  
</a>

    <span class="header__right">
      
        <nav class="menu">
  <ul class="menu__inner menu__inner--desktop">
    
      
        
          <li><a href="/about/">about</a></li>
        
      
        
          <li><a href="/research/">research</a></li>
        
      
        
          <li><a href="/software/">software</a></li>
        
      
        
          <li><a href="/posts/">posts</a></li>
        
      
    
  </ul>

  <ul class="menu__inner menu__inner--mobile">
    
      
        <li><a href="/about/">about</a></li>
      
    
      
        <li><a href="/research/">research</a></li>
      
    
      
        <li><a href="/software/">software</a></li>
      
    
      
        <li><a href="/posts/">posts</a></li>
      
    
  </ul>
</nav>

        <span class="menu-trigger">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M0 0h24v24H0z" fill="none" />
            <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z" />
          </svg>
        </span>
      
      <span class="theme-toggle">
        <svg
  class="theme-toggler"
  width="24"
  height="24"
  viewBox="0 0 48 48"
  fill="none"
  xmlns="http://www.w3.org/2000/svg"
>
  <path
    d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"
  />
</svg>

      </span>
    </span>
  </span>
</header>


      <div class="content">
        
  
  

  <div class="post">
    <h1 class="post-title">Desktop Elements Detection Using Deep Learning</h1>
    <div class="post-meta">
      
        <span class="post-date">
          2021-01-21
        </span>

        
          
        
      

      


      
    </div>

    
      <span class="post-tags">
        
          <a href="https://asiffer.github.io/tags/computer-vision/">#computer vision</a>&nbsp;
        
          <a href="https://asiffer.github.io/tags/pytorch/">#pytorch</a>&nbsp;
        
          <a href="https://asiffer.github.io/tags/desker/">#desker</a>&nbsp;
        
          <a href="https://asiffer.github.io/tags/ai/">#AI</a>&nbsp;
        
      </span>
    

    

    <div class="post-content">
      
        <h2>Table of Contents</h2>
        <aside class="table-of-contents"><nav id="TableOfContents">
  <ul>
    <li><a href="#context">Context</a></li>
    <li><a href="#strategy">Strategy</a>
      <ul>
        <li><a href="#convolutional-neural-networks-cnn">Convolutional Neural Networks (CNN)</a></li>
        <li><a href="#model-selection">Model Selection</a></li>
      </ul>
    </li>
    <li><a href="#tailoring-faster-r-cnn-to-icon-detection">Tailoring Faster-R-CNN to icon detection</a>
      <ul>
        <li><a href="#fine-tuning">Fine-tuning</a></li>
        <li><a href="#dedicated-dataset">Dedicated dataset</a></li>
      </ul>
    </li>
    <li><a href="#training-in-practice">Training in practice</a>
      <ul>
        <li><a href="#data-loader">Data-loader</a></li>
        <li><a href="#optimizer-and-hyper-parameters">Optimizer and hyper-parameters</a></li>
        <li><a href="#training-pattern">Training pattern</a></li>
      </ul>
    </li>
    <li><a href="#results">Results</a></li>
    <li><a href="#conclusion">Conclusion</a></li>
    <li><a href="#extra-exploring-gradient-accumulation">Extra: exploring gradient accumulation</a></li>
    <li><a href="#sources">Sources</a></li>
  </ul>
</nav></aside>
      
      <hr>
<p><strong>Notes</strong></p>
<p>The original post (written with Joseph Paillard) has been published on the <a href="https://blog.amossys.fr/desktop-elements-detection-using-deep-learning.html">Amossys blog</a>. Here, some elements related to the Amossys company (and not
paramount on this blog) have been removed.</p>
<hr>
<!-- raw HTML omitted -->
<p>
  <img src="/images/desker/output3.gif"  class="left"  />


<div class="caption">
    Prediction sample
</div></p>
<h2 id="context">Context</h2>
<p>This project has been carried out as part of the <a href="https://www.challenge-ia-ecw.eu/">AI &amp; Cyber challenge</a> that took place during the European Cyber Week (ECW) 2020.
This year, the challenge was to emulate human behavior on a computer.
For that purpose, each team had to develop an agent that interacts with a machine via the VNC protocol (no running agent allowed on the target).
Programs were then judged on the visual performance they provided and whether they tricked the jury or not.</p>
<p>This specific project was initially aiming at developing an autonomous agent capable of browsing a file explorer. Yet, we rapidly decided to consider the more general task of <strong>desktop component detection</strong>, which also embraces the detection of close buttons, mail icons etc. The main constraint was that the solution had to be generic, i.e. able to handle changes of Linux distribution and theme.</p>
<h2 id="strategy">Strategy</h2>
<p>Among the 5 human senses, sight is the most used during computer activity. Therefore, in order to emulate human behavior, it is relevant to try to replicate the human visual perception of the computer.
Computer vision, a sub-field of artificial intelligence, was born in the 1960s to tackle this issue.</p>
<p>The goal is to <strong>identify desktop objects on screenshots</strong> along with their position (in order to generate a browsing activity).
Among computer vision sub-fields, object detection is the most relevant since boxes are sufficient to segment the desktop components (geometrical shaped).</p>
<p>
  <img src="/images/desker/computer_vision_sub-fields.png"  class="center"  style="width:70%"  />


<div class="caption">
    Computer vision sub-fields
</div></p>
<p>Methods for object detection generally fall into deep-learning-based
approaches and are typically based on Convolutional Neural Networks (CNN).
The following section describes what makes CNN different from other
neural networks and why they are so efficient for computer vision tasks.</p>
<h3 id="convolutional-neural-networks-cnn">Convolutional Neural Networks (CNN)</h3>
<p>A CNN is a deep neural network highly effective for computer vision tasks. The architecture of CNNs is bio-inspired and emulates the organization of the visual cortex. It is made of an assembly of convolution layers, pooling layers and fully connected ones.</p>
<p>A basic neural network is generally made up of a series of fully connected layers. In such layers each neuron is connected to all the neurons of the next layer and a weight is associated to each binding.
These layers are used to learn features by changing the weights of the bindings between neurons during the training. But such networks turn out to be irrelevant for computer vision tasks for two main reasons :</p>
<ol>
<li>It requires a huge architecture to consider every pixel</li>
<li>It removes spatial dependency between pixels</li>
</ol>
<p>This is where convolution comes in, it aims at extracting dominant features that will help to characterize objects (e.g. edge detection, color detectionâ€¦ ). For that purpose, a filter is applied throughout the input image by applying matrix multiplications on small regions of the image and moving the next one with a certain stride.</p>
<p>
  <img src="/images/desker/convolution1.gif"  class="center"  />


<div class="caption">
    Convolution process example
</div></p>
<p>The output of the convolution layer is called the feature map. Its size can be either smaller than the input or larger which is useful to decrease the computational power required. More complex transformations are also applied later on in the network (e.g. pooling, for more information refer to the sources section) but the key asset of CNNs is that hardly any pre-processing is required since it is handled by the convolution layers.</p>
<h3 id="model-selection">Model Selection</h3>
<p>Even among CNN based algorithms, a wide range of architecture variations exists. They differ on accuracy, speed but also on computational power requirements which is crucial when working with resource constraints. All these aspects have to be taken into account so as to choose the most adapted algorithm.</p>
<p>Three models stand out in object detection regarding speed and precision:</p>
<ol>
<li>Region Proposals (Faster-R-CNN)</li>
<li>You Only Look Once (YOLO)</li>
<li>Single Shot MultiBox Detector (SSD)</li>
</ol>
<p>Basically YOLO is the fastest while Faster-R-CNN is the most accurate (SSD lies in between).
You can take a look at <a href="https://medium.com/@jonathan_hui/object-detection-speed-and-accuracy-comparison-faster-r-cnn-r-fcn-ssd-and-yolo-5425656ae359">this post</a> for a deeper analysis.
In our approach, we have mainly worked with Faster-R-CNN since
accuracy seemed to be the most relevant feature. Having a couple of
predictions per second is enough to mimic human behaviour.</p>
<p>From the hardware point of view, a reasonable size GPU (GeForce GTX 1660 with 6 GB of RAM) allows us to load the model and to provide inferences at a speed of approximately <strong>5fps</strong>.</p>
<h2 id="tailoring-faster-r-cnn-to-icon-detection">Tailoring Faster-R-CNN to icon detection</h2>
<p>Once a model has been selected it is not yet ready do be used. It still needs to be trained to the icon detection task as computer vision models are usually trained and evaluated on other kinds of objects (cats, dogs, pedestrians, cars&hellip;).</p>
<p>The problem is that training a computer vision model from scratch requires a huge dataset (millions of images) and a tremendous computational power, which is out of range for the vast majority of users. But all hope of success is not lost: <strong>transfer learning</strong> allows to train a model with the slightest effort by taking advantage of pre-trained models.</p>
<h3 id="fine-tuning">Fine-tuning</h3>
<p>Fine-tuning is a quite efficient technique to tailor Faster R-CNN to our icon detection task.
Instead of initializing the model randomly, we start from a model that has been pre-trained on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories).
At this stage, the model is very efficient for common object detection. Training is then performed on the final layers while early ones are frozen.
This is motivated by the observation that the early layers of a CNN contain more generic features (e.g. edge detectors or color detectors) that are useful to many tasks. Later layers of the CNN become progressively more specific to the details of the classes contained in the original dataset.
For more details about Faster R-CNN architecture, refer to <a href="https://arxiv.org/abs/1506.01497">original paper</a>.
As a result, the number of parameters to optimize is widely reduced and frozen ones are already close to their optimal value.</p>
<p>In PyTorch, a pre-trained Faster R-CNN model (with a resnet50 backbone) can easily be loaded using the <code>torchvision</code> package:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> torchvision

model <span style="color:#f92672">=</span> torchvision<span style="color:#f92672">.</span>models<span style="color:#f92672">.</span>detection<span style="color:#f92672">.</span>fasterrcnn_resnet50_fpn(pretrained<span style="color:#f92672">=</span>True)
</code></pre></div><p>Obviously, this model is made for a given number of classes. When you want to adapt the algorithm to a different number of classes, you have to modify the last layer:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> torchvision.models.detection <span style="color:#f92672">import</span> fasterrcnn_resnet50_fpn
<span style="color:#f92672">from</span> torchvision.models.detection.faster_rcnn <span style="color:#f92672">import</span> FasterRCNN, FastRCNNPredictor

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_model</span>(num_classes: int) <span style="color:#f92672">-&gt;</span> FasterRCNN:
    <span style="color:#75715e"># load an instance segmentation model pre-trained on COCO</span>
    model <span style="color:#f92672">=</span> fasterrcnn_resnet50_fpn(pretrained<span style="color:#f92672">=</span>True)

    <span style="color:#75715e"># get the number of input features for the classifier</span>
    in_features <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>roi_heads<span style="color:#f92672">.</span>box_predictor<span style="color:#f92672">.</span>cls_score<span style="color:#f92672">.</span>in_features

    <span style="color:#75715e"># replace the pre-trained head with a new one</span>
    model<span style="color:#f92672">.</span>roi_heads<span style="color:#f92672">.</span>box_predictor <span style="color:#f92672">=</span> FastRCNNPredictor(in_features, num_classes)

    <span style="color:#66d9ef">return</span> model
</code></pre></div><p>To check which parameters will be optimized, we can have a look
to their <code>require_grad</code> attribute.
Each parameter that has <code>require_grad</code> set to <code>True</code> will be updated whenever the optimizer function is called (i.e. a gradient descent step is performed). In our case it is easy to check which part of the model is frozen:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">for</span> name <span style="color:#f92672">in</span> model<span style="color:#f92672">.</span>named_parameters():
    <span style="color:#66d9ef">if</span> param<span style="color:#f92672">.</span>requires_grad:
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;{0} requires grad&#34;</span><span style="color:#f92672">.</span>format(name))
    <span style="color:#66d9ef">else</span>:
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;{0} doesn&#39;t requires grad&#34;</span><span style="color:#f92672">.</span>format(name))
</code></pre></div><pre><code class="language-console" data-lang="console">backbone.body.layer1.X.convX.weight doesn't requires grad
backbone.body.layer2.X.convX.weight requires grad
...
rpn.head.conv.weight requires grad
roi_heads.box_head.XXX requires grad
roi_heads.box_predictor.bbox_pred.weight requires grad
</code></pre><p>Here we can see that the first convolution layer is frozen while next ones are modified during the fine-tuning step.</p>
<h3 id="dedicated-dataset">Dedicated dataset</h3>
<p>A relevant dataset is often the keystone of success in machine learning. In particular, identifying every object on the screenshot is paramount.
Even though fine-tuning requires a minimum amount of data, an effort is required to create the dataset for our purpose.</p>
<p>First, several screenshots (about 100) from various [Linux] environments have been taken. Then, each image has been annotated with bounding boxes related to the classes we want to detect (about 10 at the beginning).
For image annotation, we use <a href="http://www.robots.ox.ac.uk/~vgg/software/via/">VGG annotator</a>, which is a simple and easy-to-use HTML application (see below).</p>
<p>
  <img src="/images/desker/via.png"  class="center"  />


<div class="caption">
    VGG annotator: the user selects the objects by drawing a rectangular box around it and the app generates a json file containing the annotations
</div></p>
<p>Once the images are treated, we can export the annotations to a <code>json</code> file which is structured as follows.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">...</span>
<span style="color:#e6db74">&#34;0061_screenshot.png708446&#34;</span>: {
    <span style="color:#e6db74">&#34;file_attributes&#34;</span>: {
        <span style="color:#e6db74">&#34;distribution&#34;</span>: <span style="color:#e6db74">&#34;ubuntu&#34;</span>
    },
    <span style="color:#e6db74">&#34;filename&#34;</span>: <span style="color:#e6db74">&#34;0061_screenshot.png&#34;</span>,
    <span style="color:#e6db74">&#34;regions&#34;</span>: [
    {
        <span style="color:#e6db74">&#34;shape_attributes&#34;</span>: {
            <span style="color:#e6db74">&#34;name&#34;</span>: <span style="color:#e6db74">&#34;rect&#34;</span>,
            <span style="color:#e6db74">&#34;x&#34;</span>: <span style="color:#ae81ff">111</span>,
            <span style="color:#e6db74">&#34;y&#34;</span>: <span style="color:#ae81ff">36</span>,
            <span style="color:#e6db74">&#34;width&#34;</span>: <span style="color:#ae81ff">47</span>,
            <span style="color:#e6db74">&#34;height&#34;</span>: <span style="color:#ae81ff">42</span>
        },
        <span style="color:#e6db74">&#34;region_attributes&#34;</span>: {
            <span style="color:#e6db74">&#34;Icon&#34;</span>: <span style="color:#e6db74">&#34;folder&#34;</span>
        }
    },
    {
        <span style="color:#e6db74">&#34;shape_attributes&#34;</span>: {
            <span style="color:#e6db74">&#34;name&#34;</span>: <span style="color:#e6db74">&#34;rect&#34;</span>,
            <span style="color:#e6db74">&#34;x&#34;</span>: <span style="color:#ae81ff">14</span>,
            <span style="color:#e6db74">&#34;y&#34;</span>: <span style="color:#ae81ff">109</span>,
            <span style="color:#e6db74">&#34;width&#34;</span>: <span style="color:#ae81ff">45</span>,
            <span style="color:#e6db74">&#34;height&#34;</span>: <span style="color:#ae81ff">45</span>
        },
        <span style="color:#e6db74">&#34;region_attributes&#34;</span>: {
            <span style="color:#e6db74">&#34;Icon&#34;</span>: <span style="color:#e6db74">&#34;folder&#34;</span>
        }
    },
 <span style="color:#f92672">...</span>
 ],
</code></pre></div><p>We can easily identify the bounding boxes (<code>shape_attributes</code>) and their label (<code>region_attributes</code>).</p>
<h2 id="training-in-practice">Training in practice</h2>
<p>To train Faster-RCNN on our custom dataset we need two elements:</p>
<ul>
<li>a data-loader to feed data into the CNN</li>
<li>an optimizer to modify the CNN parameters so as to perform the best detection</li>
</ul>
<h3 id="data-loader">Data-loader</h3>
<p>To feed some data to our Faster R-CNN, we mainly need a class respecting a basic interface.
It must bridge between the <code>json</code> given by VIA and the <code>pytorch</code> dataset interface.
In our case, its structure is given below.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ScreenDataset</span>:
    <span style="color:#66d9ef">def</span> __init__(self,
                 root: str,
                 transforms: Callable,
                 json_file: str):
        <span style="color:#f92672">...</span>
 
    <span style="color:#66d9ef">def</span> __getitem__(self, idx : int):
        <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">        Return the idx-th image (internally transformed) and its annotations (n boxes), 
</span><span style="color:#e6db74">        following the following structure:
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">        target = {
</span><span style="color:#e6db74">            &#39;boxes&#39;: [n x 4 tensor], 
</span><span style="color:#e6db74">            &#39;labels&#39;: [n x 1 tensor],
</span><span style="color:#e6db74">            &#39;scores&#39;: [n x 1 tensor],
</span><span style="color:#e6db74">        }
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        <span style="color:#f92672">...</span>
        <span style="color:#66d9ef">return</span> img, target
 
    <span style="color:#66d9ef">def</span> __len__(self):
        <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">        Return the number of images in the 
</span><span style="color:#e6db74">        root directory
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        <span style="color:#f92672">...</span>

</code></pre></div><p>The initialization function checks the images located in the given <code>root</code> directory.
The provided <code>transforms</code> function is a common tool to correctly shape the input image to the CNN.
Finally, the <code>json_file</code> is the annotation file.</p>
<p>This object needs two additional methods to be wrapped in a more generic PyTorch <code>DataLoader</code> (allowing batch selection, shuffling etc.).
In details, the function <code>__getitem__</code> selects an image, transforms it and parses the json file to output the
annotated boxes (<code>target</code>).</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">dataset <span style="color:#f92672">=</span> ScreenDataset(
    root<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;/path/to/images&#34;</span>,
    transforms<span style="color:#f92672">=</span>fun,
    json_file<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;/path/to/file.json&#34;</span>)

data_loader <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>DataLoader(
    dataset<span style="color:#f92672">=</span>dataset,
    batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>,
    shuffle<span style="color:#f92672">=</span>True)
</code></pre></div><h3 id="optimizer-and-hyper-parameters">Optimizer and hyper-parameters</h3>
<p>PyTorch proposes <a href="https://pytorch.org/docs/stable/optim.html">several optimizers</a>.
In our case, we use the classical stochastic gradient descent (SGD) which gives rather good results.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># get the parameters to optimize</span>
params <span style="color:#f92672">=</span> [p <span style="color:#66d9ef">for</span> p <span style="color:#f92672">in</span> model<span style="color:#f92672">.</span>parameters() <span style="color:#66d9ef">if</span> p<span style="color:#f92672">.</span>requires_grad]
<span style="color:#75715e">#Â init the oprimizer</span>
optimizer <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>SGD(params, lr<span style="color:#f92672">=</span><span style="color:#ae81ff">0.005</span>, momentum<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>)
</code></pre></div><p>In Machine learning, the training phase aims at optimizing the parameters of a model. Taking a step back, the training process also has parameters that can be optimized in order to improve the efficiency of the training. These are called hyper-parameters and are usually defined by the user.</p>
<p>First of all, it should be understood that this choice is likely to be constrained by computational power,
data and time limitation.
The three main hyper-parameters that can be adapted are the following:</p>
<ol>
<li><strong>number of epochs</strong>: The number of forward passes for each image of the dataset</li>
<li><strong>batch size</strong>: the number of training samples (here images) to work through before performing a backward pass.</li>
<li><strong>learning rate</strong>: the size of the step taken at each gradient descent.</li>
</ol>
<p>The number of epochs depends on the dataset and the time you have to train the model.
If you have a very rich dataset, a small number of epochs can be enough to train a good model.
However, the model will overfit at a moment if it learns too much from the training set (the model cannot generalize).</p>
<p>In order to obtain a consistent approximation of the gradient of the loss function, it must be calculated over a large enough batch. If the batch size is too small, gradient descent steps are more exposed to randomness and training may thus be longer.
Nevertheless, with a smaller batch size, the memory of the GPU is less monopolized and the parameters of the model are updated more frequently (which may therefore reduce the training duration).
Hence, a trade-off has to be found between precision (higher with larger batch size) and speed (shorter with frequent parameters updates and thus smaller batch size).</p>
<p>Finally, the learning rate is a pure optimization parameter as it corresponds to the step length taken after every batch.
A small value means that the optimization is cautious (slow but sure) although a high value tends to accelerate the convergence with a risk to move away from optimum regions.</p>
<p>Other parameters also exist and should be carefully chosen.</p>
<h3 id="training-pattern">Training pattern</h3>
<p>Now we have all the material to train our CNN. This stage includes 4 stages:</p>
<ol>
<li>forward pass (prediction)</li>
<li>evaluation (loss calculation)</li>
<li>backward pass (gradients calculation)</li>
<li>optimizer step (gradient descent)</li>
</ol>
<p>
  <img src="/images/desker/classic_training.png"  class="center"  style="width:40%"  />


<div class="caption">
    PyTorch training diagram
</div></p>
<p>In practice, we merely need to loop over the data. A single epoch may look like this:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">for</span> images, targets <span style="color:#f92672">in</span> data_loader:
    <span style="color:#75715e"># here we have a batch of images</span>
    <span style="color:#75715e"># and the bounding boxes (targets)</span>

    <span style="color:#75715e"># zero the parameter gradients</span>
    optimizer<span style="color:#f92672">.</span>zero_grad()

    <span style="color:#75715e"># compute the loss (forward pass)</span>
    loss <span style="color:#f92672">=</span> model(images, targets)
    <span style="color:#75715e"># gradient backpropagation</span>
    loss<span style="color:#f92672">.</span>backward()
    <span style="color:#75715e"># weights update</span>
    optimizer<span style="color:#f92672">.</span>step()
</code></pre></div><p>Now, lets zoom in on Faster R-CNN:</p>
<ul>
<li>During the <strong>forward pass</strong>, the model is fed with a batch of images and returns the objects detected in those images along with their bounding boxes and a confidence score for each prediction. The output looks like this:</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">model<span style="color:#f92672">.</span>eval()
<span style="color:#66d9ef">print</span>(model(img))

{<span style="color:#e6db74">&#39;boxes&#39;</span>: tensor([[<span style="color:#ae81ff">1668.7316</span>, <span style="color:#ae81ff">271.4885</span>, <span style="color:#ae81ff">1753.3470</span>, <span style="color:#ae81ff">359.6610</span>],
[ <span style="color:#ae81ff">973.9626</span>, <span style="color:#ae81ff">402.9771</span>, <span style="color:#ae81ff">1059.8567</span>, <span style="color:#ae81ff">490.3125</span>],
[<span style="color:#ae81ff">1669.7742</span>, <span style="color:#ae81ff">403.1881</span>, <span style="color:#ae81ff">1754.4435</span>, <span style="color:#ae81ff">490.4958</span>],
[ <span style="color:#ae81ff">973.8992</span>, <span style="color:#ae81ff">271.6441</span>, <span style="color:#ae81ff">1059.7217</span>, <span style="color:#ae81ff">359.6740</span>]], device<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cuda:0&#39;</span>), 
<span style="color:#e6db74">&#39;labels&#39;</span>: tensor([<span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">4</span>], device<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cuda:0&#39;</span>),
<span style="color:#e6db74">&#39;scores&#39;</span>: tensor([<span style="color:#ae81ff">0.9905</span>, <span style="color:#ae81ff">0.6706</span>, <span style="color:#ae81ff">0.4652</span>, <span style="color:#ae81ff">0.3768</span>], device<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cuda:0&#39;</span>)}
</code></pre></div><ul>
<li>Afterwards the loss is calculated. A <strong>loss function</strong> takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target. The relevant loss function for object detection is intersection over union (IoU): it compares the ground truth object position with the predicted bounding box.

  <img src="/images/desker/IoU.png"  class="center"  style="width:40%"  />

<div class="caption">
    Intersection over Union
</div></li>
<li>During the <strong>backward pass</strong>, the gradient of the loss function is computed with back propagation.</li>
<li>Finally the optimizer performs a <strong>gradient descent</strong> in order to optimize the parameters of the model (SGD).</li>
</ul>
<p>Our model needs about 10-15 minutes to train (100 images, 15 epochs, 2 images batch) with a GTX 1060 6GB.</p>
<h2 id="results">Results</h2>
<p>Since the ECW challenge, we have been developing the <a href="https://gitlab.com/d3sker/desker"><strong>desker</strong></a> library that aims
at implementing several desktop elements detection routines.
Our Faster R-CNN model is available through this library, so you can test it!</p>
<p>Here you can look at the results given by our trained model on various test images.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="/images/desker/result01.png"><img src="/images/desker/result01.png" alt=""></a></td>
<td><a href="/images/desker/result04.png"><img src="/images/desker/result04.png" alt=""></a></td>
</tr>
<tr>
<td><a href="/images/desker/result02.png"><img src="/images/desker/result02.png" alt=""></a></td>
<td><a href="/images/desker/result05.png"><img src="/images/desker/result05.png" alt=""></a></td>
</tr>
<tr>
<td><a href="/images/desker/result03.png"><img src="/images/desker/result03.png" alt=""></a></td>
<td><a href="/images/desker/result06.png"><img src="/images/desker/result06.png" alt=""></a></td>
</tr>
</tbody>
</table>
<h2 id="conclusion">Conclusion</h2>
<p>Computer vision is certainly a useful tool to identify relevant objects
on a screen. This step is fundamental to generate human-like computer activity.</p>
<p>Moreover, object detection can be associated with optical character recognition (OCR) and
other image processing techniques to analyze the text on the same screenshots.
Such information could produce even more realistic actions
(browsing specific folders, clicking on links&hellip;).
Some of these methods can notably be found in <a href="https://gitlab.com/d3sker/desker">desker</a>.</p>
<h2 id="extra-exploring-gradient-accumulation">Extra: exploring gradient accumulation</h2>
<p>Even with the slightest training phase, several memory issues may arise and hamper the training process.
This section introduces a solution to overcome the famous &ldquo;out of memory&rdquo; issue that occurs when the GPU lacks RAM.</p>
<p>It is worth noticing that the batch size corresponds to the number of images that will be loaded simultaneously on the GPU.
Therefore besides slowing down the training process, a limited computational power also prevents from reaching a good approximation of the gradient and thus hampers the quality of the training.
However the gradient accumulation method allows to make up for the lack of RAM. To cope with the constrained memory, a given batch is divided in mini-batches which are loaded sequentially on the GPU and forward passed in the model.
Then, after each step, the gradient is calculated with back propagation <em>but</em> the parameters of the model are only updated after a given number of steps (equal to the number of mini-batches in a batch).
Even if the sum of the gradients is clearly not equal to the gradient of the larger batch, it offers a better approximation while avoiding out of memory errors.</p>
<p>
  <img src="/images/desker/gradient_acc_training.png"  class="center"  style="width:40%"  />


<div class="caption">
    Training process with gradient accumulation
</div></p>
<p>To experiment it we used the following hyper-parameters.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<table>
<thead>
<tr>
<th>Hyper-parameter</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>batch size</td>
<td>3</td>
</tr>
<tr>
<td>number of epochs</td>
<td>10</td>
</tr>
<tr>
<td>learning rate</td>
<td>0.005</td>
</tr>
<tr>
<td>learning decay</td>
<td>0.0005</td>
</tr>
</tbody>
</table>
<p>The <strong>learning decay</strong> is a hyper-parameter that reduces the size
of the gradient descent step after each epoch adjust the learning
speed (at the end of the learning, only tiny modifications are required).
Results are presented below.</p>
<p>
  <img src="/images/desker/loss_track_withoutAG.png"  class="left"  style="float: left; width: 50%;"  />



  <img src="/images/desker/loss_track_withAG.png"  class="left"  style="float: right; width: 50%;"  />

</p>
<p>With such hyper-parameters gradient accumulation doesn&rsquo;t seem to make any difference, chances are that it is due to the small size of the dataset. Nevertheless, the model converges and makes satisfying predictions.</p>
<h2 id="sources">Sources</h2>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">https://en.wikipedia.org/wiki/Convolutional_neural_network</a></li>
<li><a href="https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53">https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53</a></li>
<li><a href="https://medium.com/@jonathan_hui/object-detection-speed-and-accuracy-comparison-faster-r-cnn-r-fcn-ssd-and-yolo-5425656ae359">https://medium.com/@jonathan_hui/object-detection-speed-and-accuracy-comparison-faster-r-cnn-r-fcn-ssd-and-yolo-5425656ae359</a></li>
<li><a href="https://gitlab.com/d3sker/desker">https://gitlab.com/d3sker/desker</a></li>
</ul>

    </div>
    
      
        <div class="pagination">
          <div class="pagination__title">
            <span class="pagination__title-h"
              >Read other posts</span
            >
            <hr />
          </div>
          <div class="pagination__buttons">
            
            
              <span class="button next">
                <a href="https://asiffer.github.io/posts/reverse-ssh/">
                  <span class="button__text">Reverse ssh tunnel</span>
                  <span class="button__icon">â†’</span>
                </a>
              </span>
            
          </div>
        </div>
      
    

    
      
        

      
    
  </div>

      </div>

      
        <footer class="footer">
  <div class="footer__inner">
    
      <a
  href="/"
  class="logo"
  style="text-decoration: none;"
>
  
    <span class="logo__mark"><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44">
  <path fill="none" d="M15 8l14.729 14.382L15 35.367" />
</svg>
</span>
    <span class="logo__text"
      >cd /home/âˆ‡</span
    >
    <span class="logo__cursor"></span>
  
</a>

      <div class="copyright">
        <span
          >Â© 2021 Powered by
          <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a></span
        >
        <span
          >Theme created by
          <a href="https://twitter.com/panr" target="_blank" rel="noopener">panr</a></span
        >
      </div>
    
  </div>
</footer>

<script src="https://asiffer.github.io/assets/main.js"></script>
<script src="https://asiffer.github.io/assets/prism.js"></script>


      
    </div>

    
  </body>
</html>
