<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Alban Siffer</title>
        <link>https://asiffer.github.io/posts/</link>
        <description>Recent content in Posts on Alban Siffer</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Thu, 21 Jan 2021 00:00:00 +0000</lastBuildDate>
        <atom:link href="https://asiffer.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>Desktop Elements Detection Using Deep Learning</title>
            <link>https://asiffer.github.io/posts/desktop-elements-detection-using-deep-learning/</link>
            <pubDate>Thu, 21 Jan 2021 00:00:00 +0000</pubDate>
            
            <guid>https://asiffer.github.io/posts/desktop-elements-detection-using-deep-learning/</guid>
            <description>Notes
The original post (written with Joseph Paillard) has been published on the Amossys blog. Here, some elements related to the Amossys company (and not paramount on this blog) have been removed.
  Prediction sample 
Context This project has been carried out as part of the AI &amp;amp; Cyber challenge that took place during the European Cyber Week (ECW) 2020. This year, the challenge was to emulate human behavior on a computer.</description>
            <content type="html"><![CDATA[<hr>
<p><strong>Notes</strong></p>
<p>The original post (written with Joseph Paillard) has been published on the <a href="https://blog.amossys.fr/desktop-elements-detection-using-deep-learning.html">Amossys blog</a>. Here, some elements related to the Amossys company (and not
paramount on this blog) have been removed.</p>
<hr>
<!-- raw HTML omitted -->
<p>
    <img src="/images/desker/output3.gif"  class="left"  />


<div class="caption">
    Prediction sample
</div></p>
<h2 id="context">Context</h2>
<p>This project has been carried out as part of the <a href="https://www.challenge-ia-ecw.eu/">AI &amp; Cyber challenge</a> that took place during the European Cyber Week (ECW) 2020.
This year, the challenge was to emulate human behavior on a computer.
For that purpose, each team had to develop an agent that interacts with a machine via the VNC protocol (no running agent allowed on the target).
Programs were then judged on the visual performance they provided and whether they tricked the jury or not.</p>
<p>This specific project was initially aiming at developing an autonomous agent capable of browsing a file explorer. Yet, we rapidly decided to consider the more general task of <strong>desktop component detection</strong>, which also embraces the detection of close buttons, mail icons etc. The main constraint was that the solution had to be generic, i.e. able to handle changes of Linux distribution and theme.</p>
<h2 id="strategy">Strategy</h2>
<p>Among the 5 human senses, sight is the most used during computer activity. Therefore, in order to emulate human behavior, it is relevant to try to replicate the human visual perception of the computer.
Computer vision, a sub-field of artificial intelligence, was born in the 1960s to tackle this issue.</p>
<p>The goal is to <strong>identify desktop objects on screenshots</strong> along with their position (in order to generate a browsing activity).
Among computer vision sub-fields, object detection is the most relevant since boxes are sufficient to segment the desktop components (geometrical shaped).</p>
<p>
    <img src="/images/desker/computer_vision_sub-fields.png"  class="center"  style="width:70%"  />


<div class="caption">
    Computer vision sub-fields
</div></p>
<p>Methods for object detection generally fall into deep-learning-based
approaches and are typically based on Convolutional Neural Networks (CNN).
The following section describes what makes CNN different from other
neural networks and why they are so efficient for computer vision tasks.</p>
<h3 id="convolutional-neural-networks-cnn">Convolutional Neural Networks (CNN)</h3>
<p>A CNN is a deep neural network highly effective for computer vision tasks. The architecture of CNNs is bio-inspired and emulates the organization of the visual cortex. It is made of an assembly of convolution layers, pooling layers and fully connected ones.</p>
<p>A basic neural network is generally made up of a series of fully connected layers. In such layers each neuron is connected to all the neurons of the next layer and a weight is associated to each binding.
These layers are used to learn features by changing the weights of the bindings between neurons during the training. But such networks turn out to be irrelevant for computer vision tasks for two main reasons :</p>
<ol>
<li>It requires a huge architecture to consider every pixel</li>
<li>It removes spatial dependency between pixels</li>
</ol>
<p>This is where convolution comes in, it aims at extracting dominant features that will help to characterize objects (e.g. edge detection, color detection… ). For that purpose, a filter is applied throughout the input image by applying matrix multiplications on small regions of the image and moving the next one with a certain stride.</p>
<p>
    <img src="/images/desker/convolution1.gif"  class="center"  />


<div class="caption">
    Convolution process example
</div></p>
<p>The output of the convolution layer is called the feature map. Its size can be either smaller than the input or larger which is useful to decrease the computational power required. More complex transformations are also applied later on in the network (e.g. pooling, for more information refer to the sources section) but the key asset of CNNs is that hardly any pre-processing is required since it is handled by the convolution layers.</p>
<h3 id="model-selection">Model Selection</h3>
<p>Even among CNN based algorithms, a wide range of architecture variations exists. They differ on accuracy, speed but also on computational power requirements which is crucial when working with resource constraints. All these aspects have to be taken into account so as to choose the most adapted algorithm.</p>
<p>Three models stand out in object detection regarding speed and precision:</p>
<ol>
<li>Region Proposals (Faster-R-CNN)</li>
<li>You Only Look Once (YOLO)</li>
<li>Single Shot MultiBox Detector (SSD)</li>
</ol>
<p>Basically YOLO is the fastest while Faster-R-CNN is the most accurate (SSD lies in between).
You can take a look at <a href="https://medium.com/@jonathan_hui/object-detection-speed-and-accuracy-comparison-faster-r-cnn-r-fcn-ssd-and-yolo-5425656ae359">this post</a> for a deeper analysis.
In our approach, we have mainly worked with Faster-R-CNN since
accuracy seemed to be the most relevant feature. Having a couple of
predictions per second is enough to mimic human behaviour.</p>
<p>From the hardware point of view, a reasonable size GPU (GeForce GTX 1660 with 6 GB of RAM) allows us to load the model and to provide inferences at a speed of approximately <strong>5fps</strong>.</p>
<h2 id="tailoring-faster-r-cnn-to-icon-detection">Tailoring Faster-R-CNN to icon detection</h2>
<p>Once a model has been selected it is not yet ready do be used. It still needs to be trained to the icon detection task as computer vision models are usually trained and evaluated on other kinds of objects (cats, dogs, pedestrians, cars&hellip;).</p>
<p>The problem is that training a computer vision model from scratch requires a huge dataset (millions of images) and a tremendous computational power, which is out of range for the vast majority of users. But all hope of success is not lost: <strong>transfer learning</strong> allows to train a model with the slightest effort by taking advantage of pre-trained models.</p>
<h3 id="fine-tuning">Fine-tuning</h3>
<p>Fine-tuning is a quite efficient technique to tailor Faster R-CNN to our icon detection task.
Instead of initializing the model randomly, we start from a model that has been pre-trained on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories).
At this stage, the model is very efficient for common object detection. Training is then performed on the final layers while early ones are frozen.
This is motivated by the observation that the early layers of a CNN contain more generic features (e.g. edge detectors or color detectors) that are useful to many tasks. Later layers of the CNN become progressively more specific to the details of the classes contained in the original dataset.
For more details about Faster R-CNN architecture, refer to <a href="https://arxiv.org/abs/1506.01497">original paper</a>.
As a result, the number of parameters to optimize is widely reduced and frozen ones are already close to their optimal value.</p>
<p>In PyTorch, a pre-trained Faster R-CNN model (with a resnet50 backbone) can easily be loaded using the <code>torchvision</code> package:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> torchvision

model <span style="color:#f92672">=</span> torchvision<span style="color:#f92672">.</span>models<span style="color:#f92672">.</span>detection<span style="color:#f92672">.</span>fasterrcnn_resnet50_fpn(pretrained<span style="color:#f92672">=</span>True)
</code></pre></div><p>Obviously, this model is made for a given number of classes. When you want to adapt the algorithm to a different number of classes, you have to modify the last layer:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> torchvision.models.detection <span style="color:#f92672">import</span> fasterrcnn_resnet50_fpn
<span style="color:#f92672">from</span> torchvision.models.detection.faster_rcnn <span style="color:#f92672">import</span> FasterRCNN, FastRCNNPredictor

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_model</span>(num_classes: int) <span style="color:#f92672">-&gt;</span> FasterRCNN:
    <span style="color:#75715e"># load an instance segmentation model pre-trained on COCO</span>
    model <span style="color:#f92672">=</span> fasterrcnn_resnet50_fpn(pretrained<span style="color:#f92672">=</span>True)

    <span style="color:#75715e"># get the number of input features for the classifier</span>
    in_features <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>roi_heads<span style="color:#f92672">.</span>box_predictor<span style="color:#f92672">.</span>cls_score<span style="color:#f92672">.</span>in_features

    <span style="color:#75715e"># replace the pre-trained head with a new one</span>
    model<span style="color:#f92672">.</span>roi_heads<span style="color:#f92672">.</span>box_predictor <span style="color:#f92672">=</span> FastRCNNPredictor(in_features, num_classes)

    <span style="color:#66d9ef">return</span> model
</code></pre></div><p>To check which parameters will be optimized, we can have a look
to their <code>require_grad</code> attribute.
Each parameter that has <code>require_grad</code> set to <code>True</code> will be updated whenever the optimizer function is called (i.e. a gradient descent step is performed). In our case it is easy to check which part of the model is frozen:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">for</span> name <span style="color:#f92672">in</span> model<span style="color:#f92672">.</span>named_parameters():
    <span style="color:#66d9ef">if</span> param<span style="color:#f92672">.</span>requires_grad:
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;{0} requires grad&#34;</span><span style="color:#f92672">.</span>format(name))
    <span style="color:#66d9ef">else</span>:
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;{0} doesn&#39;t requires grad&#34;</span><span style="color:#f92672">.</span>format(name))
</code></pre></div><pre><code class="language-console" data-lang="console">backbone.body.layer1.X.convX.weight doesn't requires grad
backbone.body.layer2.X.convX.weight requires grad
...
rpn.head.conv.weight requires grad
roi_heads.box_head.XXX requires grad
roi_heads.box_predictor.bbox_pred.weight requires grad
</code></pre><p>Here we can see that the first convolution layer is frozen while next ones are modified during the fine-tuning step.</p>
<h3 id="dedicated-dataset">Dedicated dataset</h3>
<p>A relevant dataset is often the keystone of success in machine learning. In particular, identifying every object on the screenshot is paramount.
Even though fine-tuning requires a minimum amount of data, an effort is required to create the dataset for our purpose.</p>
<p>First, several screenshots (about 100) from various [Linux] environments have been taken. Then, each image has been annotated with bounding boxes related to the classes we want to detect (about 10 at the beginning).
For image annotation, we use <a href="http://www.robots.ox.ac.uk/~vgg/software/via/">VGG annotator</a>, which is a simple and easy-to-use HTML application (see below).</p>
<p>
    <img src="/images/desker/via.png"  class="center"  />


<div class="caption">
    VGG annotator: the user selects the objects by drawing a rectangular box around it and the app generates a json file containing the annotations
</div></p>
<p>Once the images are treated, we can export the annotations to a <code>json</code> file which is structured as follows.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">...</span>
<span style="color:#e6db74">&#34;0061_screenshot.png708446&#34;</span>: {
    <span style="color:#e6db74">&#34;file_attributes&#34;</span>: {
        <span style="color:#e6db74">&#34;distribution&#34;</span>: <span style="color:#e6db74">&#34;ubuntu&#34;</span>
    },
    <span style="color:#e6db74">&#34;filename&#34;</span>: <span style="color:#e6db74">&#34;0061_screenshot.png&#34;</span>,
    <span style="color:#e6db74">&#34;regions&#34;</span>: [
    {
        <span style="color:#e6db74">&#34;shape_attributes&#34;</span>: {
            <span style="color:#e6db74">&#34;name&#34;</span>: <span style="color:#e6db74">&#34;rect&#34;</span>,
            <span style="color:#e6db74">&#34;x&#34;</span>: <span style="color:#ae81ff">111</span>,
            <span style="color:#e6db74">&#34;y&#34;</span>: <span style="color:#ae81ff">36</span>,
            <span style="color:#e6db74">&#34;width&#34;</span>: <span style="color:#ae81ff">47</span>,
            <span style="color:#e6db74">&#34;height&#34;</span>: <span style="color:#ae81ff">42</span>
        },
        <span style="color:#e6db74">&#34;region_attributes&#34;</span>: {
            <span style="color:#e6db74">&#34;Icon&#34;</span>: <span style="color:#e6db74">&#34;folder&#34;</span>
        }
    },
    {
        <span style="color:#e6db74">&#34;shape_attributes&#34;</span>: {
            <span style="color:#e6db74">&#34;name&#34;</span>: <span style="color:#e6db74">&#34;rect&#34;</span>,
            <span style="color:#e6db74">&#34;x&#34;</span>: <span style="color:#ae81ff">14</span>,
            <span style="color:#e6db74">&#34;y&#34;</span>: <span style="color:#ae81ff">109</span>,
            <span style="color:#e6db74">&#34;width&#34;</span>: <span style="color:#ae81ff">45</span>,
            <span style="color:#e6db74">&#34;height&#34;</span>: <span style="color:#ae81ff">45</span>
        },
        <span style="color:#e6db74">&#34;region_attributes&#34;</span>: {
            <span style="color:#e6db74">&#34;Icon&#34;</span>: <span style="color:#e6db74">&#34;folder&#34;</span>
        }
    },
 <span style="color:#f92672">...</span>
 ],
</code></pre></div><p>We can easily identify the bounding boxes (<code>shape_attributes</code>) and their label (<code>region_attributes</code>).</p>
<h2 id="training-in-practice">Training in practice</h2>
<p>To train Faster-RCNN on our custom dataset we need two elements:</p>
<ul>
<li>a data-loader to feed data into the CNN</li>
<li>an optimizer to modify the CNN parameters so as to perform the best detection</li>
</ul>
<h3 id="data-loader">Data-loader</h3>
<p>To feed some data to our Faster R-CNN, we mainly need a class respecting a basic interface.
It must bridge between the <code>json</code> given by VIA and the <code>pytorch</code> dataset interface.
In our case, its structure is given below.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ScreenDataset</span>:
    <span style="color:#66d9ef">def</span> __init__(self,
                 root: str,
                 transforms: Callable,
                 json_file: str):
        <span style="color:#f92672">...</span>
 
    <span style="color:#66d9ef">def</span> __getitem__(self, idx : int):
        <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">        Return the idx-th image (internally transformed) and its annotations (n boxes), 
</span><span style="color:#e6db74">        following the following structure:
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">        target = {
</span><span style="color:#e6db74">            &#39;boxes&#39;: [n x 4 tensor], 
</span><span style="color:#e6db74">            &#39;labels&#39;: [n x 1 tensor],
</span><span style="color:#e6db74">            &#39;scores&#39;: [n x 1 tensor],
</span><span style="color:#e6db74">        }
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        <span style="color:#f92672">...</span>
        <span style="color:#66d9ef">return</span> img, target
 
    <span style="color:#66d9ef">def</span> __len__(self):
        <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">        Return the number of images in the 
</span><span style="color:#e6db74">        root directory
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        <span style="color:#f92672">...</span>

</code></pre></div><p>The initialization function checks the images located in the given <code>root</code> directory.
The provided <code>transforms</code> function is a common tool to correctly shape the input image to the CNN.
Finally, the <code>json_file</code> is the annotation file.</p>
<p>This object needs two additional methods to be wrapped in a more generic PyTorch <code>DataLoader</code> (allowing batch selection, shuffling etc.).
In details, the function <code>__getitem__</code> selects an image, transforms it and parses the json file to output the
annotated boxes (<code>target</code>).</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">dataset <span style="color:#f92672">=</span> ScreenDataset(
    root<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;/path/to/images&#34;</span>,
    transforms<span style="color:#f92672">=</span>fun,
    json_file<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;/path/to/file.json&#34;</span>)

data_loader <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>DataLoader(
    dataset<span style="color:#f92672">=</span>dataset,
    batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>,
    shuffle<span style="color:#f92672">=</span>True)
</code></pre></div><h3 id="optimizer-and-hyper-parameters">Optimizer and hyper-parameters</h3>
<p>PyTorch proposes <a href="https://pytorch.org/docs/stable/optim.html">several optimizers</a>.
In our case, we use the classical stochastic gradient descent (SGD) which gives rather good results.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># get the parameters to optimize</span>
params <span style="color:#f92672">=</span> [p <span style="color:#66d9ef">for</span> p <span style="color:#f92672">in</span> model<span style="color:#f92672">.</span>parameters() <span style="color:#66d9ef">if</span> p<span style="color:#f92672">.</span>requires_grad]
<span style="color:#75715e"># init the oprimizer</span>
optimizer <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>SGD(params, lr<span style="color:#f92672">=</span><span style="color:#ae81ff">0.005</span>, momentum<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>)
</code></pre></div><p>In Machine learning, the training phase aims at optimizing the parameters of a model. Taking a step back, the training process also has parameters that can be optimized in order to improve the efficiency of the training. These are called hyper-parameters and are usually defined by the user.</p>
<p>First of all, it should be understood that this choice is likely to be constrained by computational power,
data and time limitation.
The three main hyper-parameters that can be adapted are the following:</p>
<ol>
<li><strong>number of epochs</strong>: The number of forward passes for each image of the dataset</li>
<li><strong>batch size</strong>: the number of training samples (here images) to work through before performing a backward pass.</li>
<li><strong>learning rate</strong>: the size of the step taken at each gradient descent.</li>
</ol>
<p>The number of epochs depends on the dataset and the time you have to train the model.
If you have a very rich dataset, a small number of epochs can be enough to train a good model.
However, the model will overfit at a moment if it learns too much from the training set (the model cannot generalize).</p>
<p>In order to obtain a consistent approximation of the gradient of the loss function, it must be calculated over a large enough batch. If the batch size is too small, gradient descent steps are more exposed to randomness and training may thus be longer.
Nevertheless, with a smaller batch size, the memory of the GPU is less monopolized and the parameters of the model are updated more frequently (which may therefore reduce the training duration).
Hence, a trade-off has to be found between precision (higher with larger batch size) and speed (shorter with frequent parameters updates and thus smaller batch size).</p>
<p>Finally, the learning rate is a pure optimization parameter as it corresponds to the step length taken after every batch.
A small value means that the optimization is cautious (slow but sure) although a high value tends to accelerate the convergence with a risk to move away from optimum regions.</p>
<p>Other parameters also exist and should be carefully chosen.</p>
<h3 id="training-pattern">Training pattern</h3>
<p>Now we have all the material to train our CNN. This stage includes 4 stages:</p>
<ol>
<li>forward pass (prediction)</li>
<li>evaluation (loss calculation)</li>
<li>backward pass (gradients calculation)</li>
<li>optimizer step (gradient descent)</li>
</ol>
<p>
    <img src="/images/desker/classic_training.png"  class="center"  style="width:40%"  />


<div class="caption">
    PyTorch training diagram
</div></p>
<p>In practice, we merely need to loop over the data. A single epoch may look like this:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">for</span> images, targets <span style="color:#f92672">in</span> data_loader:
    <span style="color:#75715e"># here we have a batch of images</span>
    <span style="color:#75715e"># and the bounding boxes (targets)</span>

    <span style="color:#75715e"># zero the parameter gradients</span>
    optimizer<span style="color:#f92672">.</span>zero_grad()

    <span style="color:#75715e"># compute the loss (forward pass)</span>
    loss <span style="color:#f92672">=</span> model(images, targets)
    <span style="color:#75715e"># gradient backpropagation</span>
    loss<span style="color:#f92672">.</span>backward()
    <span style="color:#75715e"># weights update</span>
    optimizer<span style="color:#f92672">.</span>step()
</code></pre></div><p>Now, lets zoom in on Faster R-CNN:</p>
<ul>
<li>During the <strong>forward pass</strong>, the model is fed with a batch of images and returns the objects detected in those images along with their bounding boxes and a confidence score for each prediction. The output looks like this:</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">model<span style="color:#f92672">.</span>eval()
<span style="color:#66d9ef">print</span>(model(img))

{<span style="color:#e6db74">&#39;boxes&#39;</span>: tensor([[<span style="color:#ae81ff">1668.7316</span>, <span style="color:#ae81ff">271.4885</span>, <span style="color:#ae81ff">1753.3470</span>, <span style="color:#ae81ff">359.6610</span>],
[ <span style="color:#ae81ff">973.9626</span>, <span style="color:#ae81ff">402.9771</span>, <span style="color:#ae81ff">1059.8567</span>, <span style="color:#ae81ff">490.3125</span>],
[<span style="color:#ae81ff">1669.7742</span>, <span style="color:#ae81ff">403.1881</span>, <span style="color:#ae81ff">1754.4435</span>, <span style="color:#ae81ff">490.4958</span>],
[ <span style="color:#ae81ff">973.8992</span>, <span style="color:#ae81ff">271.6441</span>, <span style="color:#ae81ff">1059.7217</span>, <span style="color:#ae81ff">359.6740</span>]], device<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cuda:0&#39;</span>), 
<span style="color:#e6db74">&#39;labels&#39;</span>: tensor([<span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">4</span>], device<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cuda:0&#39;</span>),
<span style="color:#e6db74">&#39;scores&#39;</span>: tensor([<span style="color:#ae81ff">0.9905</span>, <span style="color:#ae81ff">0.6706</span>, <span style="color:#ae81ff">0.4652</span>, <span style="color:#ae81ff">0.3768</span>], device<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cuda:0&#39;</span>)}
</code></pre></div><ul>
<li>Afterwards the loss is calculated. A <strong>loss function</strong> takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target. The relevant loss function for object detection is intersection over union (IoU): it compares the ground truth object position with the predicted bounding box.

    <img src="/images/desker/IoU.png"  class="center"  style="width:40%"  />

<div class="caption">
    Intersection over Union
</div></li>
<li>During the <strong>backward pass</strong>, the gradient of the loss function is computed with back propagation.</li>
<li>Finally the optimizer performs a <strong>gradient descent</strong> in order to optimize the parameters of the model (SGD).</li>
</ul>
<p>Our model needs about 10-15 minutes to train (100 images, 15 epochs, 2 images batch) with a GTX 1060 6GB.</p>
<h2 id="results">Results</h2>
<p>Since the ECW challenge, we have been developing the <a href="https://gitlab.com/d3sker/desker"><strong>desker</strong></a> library that aims
at implementing several desktop elements detection routines.
Our Faster R-CNN model is available through this library, so you can test it!</p>
<p>Here you can look at the results given by our trained model on various test images.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="/images/desker/result01.png"><img src="/images/desker/result01.png" alt=""></a></td>
<td><a href="/images/desker/result04.png"><img src="/images/desker/result04.png" alt=""></a></td>
</tr>
<tr>
<td><a href="/images/desker/result02.png"><img src="/images/desker/result02.png" alt=""></a></td>
<td><a href="/images/desker/result05.png"><img src="/images/desker/result05.png" alt=""></a></td>
</tr>
<tr>
<td><a href="/images/desker/result03.png"><img src="/images/desker/result03.png" alt=""></a></td>
<td><a href="/images/desker/result06.png"><img src="/images/desker/result06.png" alt=""></a></td>
</tr>
</tbody>
</table>
<h2 id="conclusion">Conclusion</h2>
<p>Computer vision is certainly a useful tool to identify relevant objects
on a screen. This step is fundamental to generate human-like computer activity.</p>
<p>Moreover, object detection can be associated with optical character recognition (OCR) and
other image processing techniques to analyze the text on the same screenshots.
Such information could produce even more realistic actions
(browsing specific folders, clicking on links&hellip;).
Some of these methods can notably be found in <a href="https://gitlab.com/d3sker/desker">desker</a>.</p>
<h2 id="extra-exploring-gradient-accumulation">Extra: exploring gradient accumulation</h2>
<p>Even with the slightest training phase, several memory issues may arise and hamper the training process.
This section introduces a solution to overcome the famous &ldquo;out of memory&rdquo; issue that occurs when the GPU lacks RAM.</p>
<p>It is worth noticing that the batch size corresponds to the number of images that will be loaded simultaneously on the GPU.
Therefore besides slowing down the training process, a limited computational power also prevents from reaching a good approximation of the gradient and thus hampers the quality of the training.
However the gradient accumulation method allows to make up for the lack of RAM. To cope with the constrained memory, a given batch is divided in mini-batches which are loaded sequentially on the GPU and forward passed in the model.
Then, after each step, the gradient is calculated with back propagation <em>but</em> the parameters of the model are only updated after a given number of steps (equal to the number of mini-batches in a batch).
Even if the sum of the gradients is clearly not equal to the gradient of the larger batch, it offers a better approximation while avoiding out of memory errors.</p>
<p>
    <img src="/images/desker/gradient_acc_training.png"  class="center"  style="width:40%"  />


<div class="caption">
    Training process with gradient accumulation
</div></p>
<p>To experiment it we used the following hyper-parameters.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<table>
<thead>
<tr>
<th>Hyper-parameter</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>batch size</td>
<td>3</td>
</tr>
<tr>
<td>number of epochs</td>
<td>10</td>
</tr>
<tr>
<td>learning rate</td>
<td>0.005</td>
</tr>
<tr>
<td>learning decay</td>
<td>0.0005</td>
</tr>
</tbody>
</table>
<p>The <strong>learning decay</strong> is a hyper-parameter that reduces the size
of the gradient descent step after each epoch adjust the learning
speed (at the end of the learning, only tiny modifications are required).
Results are presented below.</p>
<p>
    <img src="/images/desker/loss_track_withoutAG.png"  class="left"  style="float: left; width: 50%;"  />



    <img src="/images/desker/loss_track_withAG.png"  class="left"  style="float: right; width: 50%;"  />

</p>
<p>With such hyper-parameters gradient accumulation doesn&rsquo;t seem to make any difference, chances are that it is due to the small size of the dataset. Nevertheless, the model converges and makes satisfying predictions.</p>
<h2 id="sources">Sources</h2>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">https://en.wikipedia.org/wiki/Convolutional_neural_network</a></li>
<li><a href="https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53">https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53</a></li>
<li><a href="https://medium.com/@jonathan_hui/object-detection-speed-and-accuracy-comparison-faster-r-cnn-r-fcn-ssd-and-yolo-5425656ae359">https://medium.com/@jonathan_hui/object-detection-speed-and-accuracy-comparison-faster-r-cnn-r-fcn-ssd-and-yolo-5425656ae359</a></li>
<li><a href="https://gitlab.com/d3sker/desker">https://gitlab.com/d3sker/desker</a></li>
</ul>
]]></content>
        </item>
        
        <item>
            <title>Reverse ssh tunnel</title>
            <link>https://asiffer.github.io/posts/reverse-ssh/</link>
            <pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate>
            
            <guid>https://asiffer.github.io/posts/reverse-ssh/</guid>
            <description>In this post we explain a way to connect to a remote machine behind a NAT with a reverse ssh tunnel. Here we use a third party server as we assume that both local and distant machines are not publicly available (neither public IP nor port forwarding).
Because of NAT, we must first set-up a tunnel between the distant server and the proxy. The proxy is running a ssh server listening on port 22000 (we can connect to it with username user and a secret password):</description>
            <content type="html"><![CDATA[<p>In this post we explain a way to connect to a remote machine behind a NAT with a reverse ssh tunnel. Here we use a third
party server as we assume that both local and distant machines are not publicly available (neither public IP nor port forwarding).</p>

    <img src="/images/reverse-ssh/reverse_ssh1.png"  alt="goal"  class="center"  />


<p>Because of NAT, we must first set-up a tunnel between the distant server and the proxy.
The proxy is running a ssh server listening on port <code>22000</code> (we can connect to it with username <code>user</code> and a secret password):</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sshd -D -p <span style="color:#ae81ff">22000</span> -o <span style="color:#e6db74">&#34;AllowTcpForwarding=all&#34;</span>
</code></pre></div><p>The distant machine is also running a ssh server on port <code>10000</code>.
From the distant machine, we must set up the tunnel</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ssh -NR 22100:localhost:10000 -p <span style="color:#ae81ff">22000</span> user@server.cloud.com
</code></pre></div><p>What does this command do? Taking it from the end, it connects to the proxy server (<code>-p 22000 user@server.cloud.com</code>). Then it tells the proxy server to bind its port <code>22100</code> to the ssh server of the distant machine.</p>

    <img src="/images/reverse-ssh/reverse_ssh2.png"  alt="goal"  class="center"  />


<p>Thus, from the proxy, you can connect (with ssh) to <code>localhost:22100</code> and you will have a remote session on the distant machine (if you have the right credentials).</p>
<p>Now we just have to connect our local machine to the proxy and hop to  <code>localhost:22100</code>. Below we detail a practical example.</p>
<p>Let us imagine that our distant machine only accepts public key authentication (user <code>guest</code> with private key stored in <code>~/.ssh/distant</code>).
We can ease the connection, with the following ssh config file (<code>~/.ssh/config</code>).</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#a6e22e">Host proxy</span>
    <span style="color:#a6e22e">User user</span>
    <span style="color:#a6e22e">Port 22000</span>
    <span style="color:#a6e22e">Hostname server.cloud.com</span>

<span style="color:#a6e22e">Host distant</span>
    <span style="color:#75715e"># Connect to the host &#39;proxy&#39; </span>
    <span style="color:#75715e"># (defined above)</span>
    <span style="color:#a6e22e">ProxyCommand ssh -W %h:%p proxy</span>
    <span style="color:#75715e"># Here you can imagine that you are</span>
    <span style="color:#75715e"># on the proxy server. The following</span>
    <span style="color:#75715e"># information are used to finally </span>
    <span style="color:#75715e"># reach the distant machine.</span>
    <span style="color:#a6e22e">User guest</span>
    <span style="color:#a6e22e">Port 22100</span>
    <span style="color:#a6e22e">Hostname localhost</span>
    <span style="color:#a6e22e">IdentityFile ~/.ssh/distant</span>
</code></pre></div><p>Now we just have to type</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ssh distant
</code></pre></div><p>When you run this command, it uses the <code>ProxyCommand</code> to connect to the proxy. Briefly it binds a local port to the ssh proxy server. Then it uses the tunnel (<code>localhost:22100</code>)
to reach the distant machine: user <code>guest</code> with the locally stored authentication key  <code>~/.ssh/distant</code>.</p>
<p>In particular, this method prevents from forwarding the ssh agent to the proxy server, so it does no expose local ssh secrets on the proxy server.
Credentials are merely passed through the whole [and encrypted] tunnel (local &ndash; distant) and cannot be highjacked.</p>
]]></content>
        </item>
        
        <item>
            <title>Passing numpy array to shared library</title>
            <link>https://asiffer.github.io/posts/numpy/</link>
            <pubDate>Sun, 29 Mar 2020 00:00:00 +0000</pubDate>
            
            <guid>https://asiffer.github.io/posts/numpy/</guid>
            <description>Introduction Imagine that we have developed a C or C++ (or rust) library which does some operations on vectors and matrices (linear algebra). Without talking about performances, we chose such a language for several reasons:
 System constraints (maybe our initial target does not embed neither java nor python virtual machines) Interoperability with other software (our piece of code if a sub-module of a bigger project) Need to manage memory (yes it happens) &amp;hellip;  Testing such a library is painful.</description>
            <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Imagine that we have developed a <code>C</code> or <code>C++</code> (or <code>rust</code>) library which does some operations on vectors and matrices (linear algebra).
Without talking about performances, we chose such a language for several reasons:</p>
<ul>
<li>System constraints (maybe our initial target does not embed neither <code>java</code> nor <code>python</code> virtual machines)</li>
<li>Interoperability with other software (our piece of code if a sub-module of a bigger project)</li>
<li>Need to manage memory (yes it happens) &hellip;</li>
</ul>
<p>Testing such a library is painful. Just let us imagine that we want to test a function with input data generated from several different (and weird) distributions (writing such tests in <code>C</code> does not motivate me so much).</p>
<p>But we know that <code>python</code> can generate these data in one line (thanks to <code>numpy</code> or <code>scipy</code>). So the question is the following: <strong>How to pass <code>numpy</code> array to <code>C</code> library ?</strong></p>
<h2 id="our-c-written-shared-library">Our C written shared library</h2>
<p>First let us write a <code>C</code> library which prints a basic array.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c" data-lang="c"><span style="color:#75715e">// 
</span><span style="color:#75715e">// main.c
</span><span style="color:#75715e">// 
</span><span style="color:#75715e"></span><span style="color:#75715e">#include</span> <span style="color:#75715e">&lt;stdio.h&gt;</span><span style="color:#75715e">
</span><span style="color:#75715e"></span>
<span style="color:#66d9ef">void</span> <span style="color:#a6e22e">print_array</span>(<span style="color:#66d9ef">double</span> <span style="color:#f92672">*</span>v, size_t n)
{
    <span style="color:#66d9ef">for</span> (size_t i <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>; i <span style="color:#f92672">&lt;</span> n; i<span style="color:#f92672">++</span>)
    {
        printf(<span style="color:#e6db74">&#34;%f &#34;</span>, v[i]);
    }
    printf(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>);
}

<span style="color:#66d9ef">int</span> <span style="color:#a6e22e">main</span>()
{
}
</code></pre></div><p>We can compile it as a shared library with <code>gcc</code> for example:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ gcc -Wall -pedantic -shared -fPIC -o mylib.so main.c
</code></pre></div><p>The options <code>-Wall -pedantic</code> are artifacts of my first <code>C</code> lessons I attended :) That is to impose not compiling with warnings and also to respect some coding rules.</p>
<p>The <code>-shared</code> option is to create a shared library and the <code>-fPIC</code> flag is to make Positional Independent Code. This latter flag is a common practice while building shared library (see <a href="https://stackoverflow.com/a/967055">https://stackoverflow.com/a/967055</a> for details).</p>
<h2 id="basic-call-from-python">Basic call from python</h2>
<p><code>python</code> has many interoperability with <code>C</code>. Thanks to the built-in <code>ctypes</code> library  you can <em>easily</em> manipulate external <code>C</code> code. Let us call our printing function!</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># </span>
<span style="color:#75715e"># test.py</span>
<span style="color:#75715e"># </span>
<span style="color:#f92672">from</span> ctypes <span style="color:#f92672">import</span> CDLL, POINTER
<span style="color:#f92672">from</span> ctypes <span style="color:#f92672">import</span> c_size_t, c_double
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np

<span style="color:#75715e"># load the library</span>
mylib <span style="color:#f92672">=</span> CDLL(<span style="color:#e6db74">&#34;mylib.so&#34;</span>)

<span style="color:#75715e"># C-type corresponding to numpy array </span>
ND_POINTER_1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>ctypeslib<span style="color:#f92672">.</span>ndpointer(dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float64, 
                                      ndim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,
                                      flags<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;C&#34;</span>)

<span style="color:#75715e"># define prototypes</span>
mylib<span style="color:#f92672">.</span>print_array<span style="color:#f92672">.</span>argtypes <span style="color:#f92672">=</span> [ND_POINTER_1, c_size_t]
mylib<span style="color:#f92672">.</span>print_array<span style="color:#f92672">.</span>restype <span style="color:#f92672">=</span> None

<span style="color:#75715e"># create array X = [1 1 1 1 1]</span>
X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>ones(<span style="color:#ae81ff">5</span>)
<span style="color:#75715e"># call function</span>
mylib<span style="color:#f92672">.</span>print_array(X, X<span style="color:#f92672">.</span>size)
</code></pre></div><p>Let us detail the previous snippet. First we load the library we compiled.
Then we need to define the prototype of our function to properly call it. The problem is that we want a <code>numpy</code> array as input.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<div class="info">
    The <code>numpy</code> library is mainly written in <code>C</code>. A <code>numpy</code> array is basically a data buffer (<code>char*</code>) with some metadata (see <a href="https://scipy-lectures.org/advanced/advanced_numpy/">https://scipy-lectures.org/advanced/advanced_numpy/</a> for more information). Thus a <code>C</code> function can easily operate on its data.
</div>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>No problem, we can retrieve the backed data buffer with <code>np.ctypeslib.ndpointer</code>. We precise that
our array stores <code>np.float64</code> (double), it has a single dimension (array) and that the storage is row-major (this is not relevant here).</p>
<p>Now in the terminal:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ python3 test.py
1.000000 1.000000 1.000000 1.000000 1.000000
</code></pre></div><h2 id="enter-the-matrix">Enter the matrix</h2>
<p>It works fine! Now, what about sending matrices ? Actually this is quite the same thing since behind we still have &hellip; an array. The indexing creates this abstraction of rows and columns, but <code>numpy</code> always manage a <code>char*</code>.</p>
<p>So, let us write our new printing function:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c" data-lang="c"><span style="color:#66d9ef">void</span> <span style="color:#a6e22e">print_matrix</span>(<span style="color:#66d9ef">double</span> <span style="color:#f92672">*</span>v, size_t n, size_t p)
{
    <span style="color:#66d9ef">for</span> (size_t i <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>; i <span style="color:#f92672">&lt;</span> n; i<span style="color:#f92672">++</span>) {
        <span style="color:#66d9ef">for</span> (size_t j <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>; j <span style="color:#f92672">&lt;</span> p; j<span style="color:#f92672">++</span>) {
            printf(<span style="color:#e6db74">&#34;%f &#34;</span>, v[i <span style="color:#f92672">*</span> n <span style="color:#f92672">+</span> j]);
        }
        printf(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>);
    }
    printf(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>);
}
</code></pre></div><p>As we said, the magic lies in the indexing part: the value at <code>(i,j)</code> is located at <code>i * n + j</code> in the data buffer.
To call this function, we can add these lines to our initial code:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># C-type corresponding to numpy 2-dimensional array (matrix) </span>
ND_POINTER_2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>ctypeslib<span style="color:#f92672">.</span>ndpointer(dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float64, 
                                      ndim<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>,
                                      flags<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;C&#34;</span>)

<span style="color:#75715e"># define the prototype</span>
mylib<span style="color:#f92672">.</span>print_matrix<span style="color:#f92672">.</span>argtypes <span style="color:#f92672">=</span> [ND_POINTER_2, c_size_t]
mylib<span style="color:#f92672">.</span>print_array<span style="color:#f92672">.</span>restype <span style="color:#f92672">=</span> None
</code></pre></div><p>Here we want to send a 2-dimensional array (matrix). So we define a pointer
to such an object and we set the prototype of the <code>C</code> function. Then we can call it after creating a toy matrix.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># create matrix</span>
<span style="color:#75715e">#     | 1 2 3 |</span>
<span style="color:#75715e"># M = | 4 5 6 |</span>
<span style="color:#75715e">#     | 7 8 9 |</span>
M <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">1</span>, dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float64)<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>, order<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;C&#34;</span>)
<span style="color:#75715e"># call function (*M.shape expands the dimensions of M)</span>
mylib<span style="color:#f92672">.</span>print_matrix(M, <span style="color:#f92672">*</span>M<span style="color:#f92672">.</span>shape)
</code></pre></div><p>In the terminal it outputs:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ python3 test.py
1.000000  2.000000  3.000000  
4.000000  5.000000  6.000000  
7.000000  8.000000  9.000000
</code></pre></div><h3 id="calling-c-or-go-code">Calling <code>C++</code> or <code>Go</code> code</h3>
<p>Until now, we have called <code>C</code> code. Actually, we can call <code>C++</code> of <code>Go</code> code in the same manner. Unfortunately or fortunately, it relies on the ability of <code>C++</code>-written and <code>Go</code>-written shared library to export their functions using the <code>cdecl</code> calling convention. In a word, it is like calling <code>C</code> code.</p>
<h4 id="example-in-c">Example in <code>C++</code></h4>
<p>In <code>C++</code>, you can export function with the <code>extern &quot;C&quot;</code> declaration.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c++" data-lang="c++"><span style="color:#75715e">// main.cpp
</span><span style="color:#75715e"></span><span style="color:#75715e">#include</span> <span style="color:#75715e">&lt;iostream&gt;</span><span style="color:#75715e">
</span><span style="color:#75715e"></span>
<span style="color:#66d9ef">extern</span> <span style="color:#e6db74">&#34;C&#34;</span>
{

    <span style="color:#66d9ef">void</span> <span style="color:#a6e22e">print_array</span>(<span style="color:#66d9ef">double</span> <span style="color:#f92672">*</span>array, size_t n)
    {
        <span style="color:#66d9ef">for</span> (size_t i <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>; i <span style="color:#f92672">&lt;</span> n; i<span style="color:#f92672">++</span>)
        {
            std<span style="color:#f92672">::</span>cout <span style="color:#f92672">&lt;&lt;</span> array[i] <span style="color:#f92672">&lt;&lt;</span> <span style="color:#e6db74">&#34; &#34;</span>;
        }
        std<span style="color:#f92672">::</span>cout <span style="color:#f92672">&lt;&lt;</span> std<span style="color:#f92672">::</span>endl;
    }

    <span style="color:#66d9ef">void</span> <span style="color:#a6e22e">print_matrix</span>(<span style="color:#66d9ef">double</span> <span style="color:#f92672">*</span>array, size_t n, size_t p)
    {
        <span style="color:#66d9ef">for</span> (size_t i <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>; i <span style="color:#f92672">&lt;</span> n; i<span style="color:#f92672">++</span>)
        {
            <span style="color:#66d9ef">for</span> (size_t j <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>; j <span style="color:#f92672">&lt;</span> p; j<span style="color:#f92672">++</span>)
            {
                std<span style="color:#f92672">::</span>cout <span style="color:#f92672">&lt;&lt;</span> array[i <span style="color:#f92672">*</span> n <span style="color:#f92672">+</span> j] <span style="color:#f92672">&lt;&lt;</span> <span style="color:#e6db74">&#34; &#34;</span>;
            }
            std<span style="color:#f92672">::</span>cout <span style="color:#f92672">&lt;&lt;</span> std<span style="color:#f92672">::</span>endl;
        }
        std<span style="color:#f92672">::</span>cout <span style="color:#f92672">&lt;&lt;</span> std<span style="color:#f92672">::</span>endl;
    }
}

<span style="color:#66d9ef">int</span> <span style="color:#a6e22e">main</span>() {}
</code></pre></div><p>It naturally looks like the initial <code>C</code> code, except that we use the <code>iostream</code> library. We can compile the program with <code>gcc</code>, linking with the standard <code>C++</code> library:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ gcc -Wall -pedantic -shared -fPIC -o mycpplib.so main.cpp -lstdc++
</code></pre></div><p>The <code>python</code> code is roughly the same as before (here we must call <code>mycpplib.so</code>), so we don&rsquo;t rewrite it. Hopefully the results are the same!</p>
<p>Obviously, this example does not use the whole power of <code>C++</code>, namely object-oriented programming (OOP).
<code>C++</code> adds classes and methods which are more complex stuff than
<code>C</code> types and functions, so they cannot be exported as is. To circumvent this problem, we can basically use pointers.</p>
<h4 id="example-in-go">Example in <code>Go</code></h4>
<p><code>Go</code> is a more recent programming language which becomes increasingly prevalent.
It has many assets but the best one is its simplicity. The syntax has very few features, making it very easy to learn (1 week to to go through just about every aspect).
Obviously it has several other advantages but I won&rsquo;t detail it here.</p>
<p>The aptly named <code>C</code> package allows to communicate between <code>Go</code> and <code>C</code> programs. In particular, you can export functions but also manage the <code>C</code> types.</p>
<p>Our <code>Go</code> code can look like this:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#75715e">// main.go
</span><span style="color:#75715e"></span><span style="color:#f92672">package</span> <span style="color:#a6e22e">main</span>

<span style="color:#f92672">import</span> <span style="color:#e6db74">&#34;C&#34;</span>

<span style="color:#f92672">import</span> (
	<span style="color:#e6db74">&#34;bytes&#34;</span>
	<span style="color:#e6db74">&#34;encoding/binary&#34;</span>
	<span style="color:#e6db74">&#34;fmt&#34;</span>
	<span style="color:#e6db74">&#34;unsafe&#34;</span>
)

<span style="color:#75715e">// SIZEOF_FLOAT64 is the number of bytes behind a float64
</span><span style="color:#75715e"></span><span style="color:#66d9ef">const</span> <span style="color:#a6e22e">SIZEOF_FLOAT64</span> = <span style="color:#ae81ff">8</span>

<span style="color:#75715e">// ToFloat64Slice converts a slice of bytes in slice of float64
</span><span style="color:#75715e"></span><span style="color:#66d9ef">func</span> <span style="color:#a6e22e">ToFloat64Slice</span>(<span style="color:#a6e22e">raw</span> []<span style="color:#66d9ef">byte</span>) []<span style="color:#66d9ef">float64</span> {
    <span style="color:#75715e">// create an io.Reader from these bytes
</span><span style="color:#75715e"></span>    <span style="color:#a6e22e">buffer</span> <span style="color:#f92672">:=</span> <span style="color:#a6e22e">bytes</span>.<span style="color:#a6e22e">NewReader</span>(<span style="color:#a6e22e">raw</span>)
    <span style="color:#75715e">// init a slice of float64
</span><span style="color:#75715e"></span>    <span style="color:#a6e22e">data</span> <span style="color:#f92672">:=</span> make([]<span style="color:#66d9ef">float64</span>, len(<span style="color:#a6e22e">raw</span>)<span style="color:#f92672">/</span><span style="color:#a6e22e">SIZEOF_FLOAT64</span>)
    <span style="color:#75715e">// Read bytes and copy them into the float64 slice
</span><span style="color:#75715e"></span>    <span style="color:#66d9ef">if</span> <span style="color:#a6e22e">err</span> <span style="color:#f92672">:=</span> <span style="color:#a6e22e">binary</span>.<span style="color:#a6e22e">Read</span>(<span style="color:#a6e22e">buffer</span>, <span style="color:#a6e22e">binary</span>.<span style="color:#a6e22e">LittleEndian</span>, <span style="color:#f92672">&amp;</span><span style="color:#a6e22e">data</span>); <span style="color:#a6e22e">err</span> <span style="color:#f92672">!=</span> <span style="color:#66d9ef">nil</span> {
        <span style="color:#a6e22e">fmt</span>.<span style="color:#a6e22e">Println</span>(<span style="color:#a6e22e">err</span>)
        <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">nil</span>
    }
    <span style="color:#66d9ef">return</span> <span style="color:#a6e22e">data</span>
}

<span style="color:#75715e">//export printSlice
</span><span style="color:#75715e"></span><span style="color:#66d9ef">func</span> <span style="color:#a6e22e">printSlice</span>(<span style="color:#a6e22e">array</span> <span style="color:#f92672">*</span><span style="color:#66d9ef">float64</span>, <span style="color:#a6e22e">n</span> <span style="color:#66d9ef">int</span>) {
    <span style="color:#75715e">// load the raw array into a slice of bytes: []byte
</span><span style="color:#75715e"></span>    <span style="color:#a6e22e">raw</span> <span style="color:#f92672">:=</span> <span style="color:#a6e22e">C</span>.<span style="color:#a6e22e">GoBytes</span>(<span style="color:#a6e22e">unsafe</span>.<span style="color:#a6e22e">Pointer</span>(<span style="color:#a6e22e">array</span>), <span style="color:#a6e22e">C</span>.int(<span style="color:#a6e22e">n</span>)<span style="color:#f92672">*</span><span style="color:#a6e22e">SIZEOF_FLOAT64</span>)
    <span style="color:#a6e22e">fmt</span>.<span style="color:#a6e22e">Println</span>(<span style="color:#a6e22e">ToFloat64Slice</span>(<span style="color:#a6e22e">raw</span>))
}

<span style="color:#66d9ef">func</span> <span style="color:#a6e22e">main</span>() {}
</code></pre></div><p>Here the difference is that the buffer is copied into a <code>Go</code> structure.
In fact <code>Go</code> provides some idiomatic functions to convert data between <code>C</code>.
<code>C</code> arrays are handled as <code>Go</code> slices of bytes (<code>[]byte</code>) with the function <code>C.GoBytes</code>. However, it would seem that we have to copy these bytes to see them as <code>float64</code> (function <code>ToFloat64Slice</code>).</p>
<p>To compile this file to a shared library we have define the build mode:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ go build -buildmode<span style="color:#f92672">=</span>c-shared -o mygolib.so main.go
</code></pre></div><p>It also creates a header you can include in your <code>C/C++</code> project to use
the functions.</p>
<p>Finally, there is a ugly trick not to copy data. We can cast the <code>C</code> array
into a <code>Go</code> array (seems not so ugly ?!).</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#66d9ef">const</span> <span style="color:#a6e22e">MAX_SIZE</span> = <span style="color:#ae81ff">1024</span>

<span style="color:#75715e">//export printSliceUgly
</span><span style="color:#75715e"></span><span style="color:#66d9ef">func</span> <span style="color:#a6e22e">printSliceUgly</span>(<span style="color:#a6e22e">cArray</span> <span style="color:#f92672">*</span><span style="color:#66d9ef">float64</span>, <span style="color:#a6e22e">n</span> <span style="color:#66d9ef">int</span>) {
    <span style="color:#75715e">// cast C pointer to pointer to a Go array
</span><span style="color:#75715e"></span>    <span style="color:#a6e22e">goArray</span> <span style="color:#f92672">:=</span> (<span style="color:#f92672">*</span>[<span style="color:#a6e22e">MAX_SIZE</span>]<span style="color:#66d9ef">float64</span>)(<span style="color:#a6e22e">unsafe</span>.<span style="color:#a6e22e">Pointer</span>(<span style="color:#a6e22e">cArray</span>))
    <span style="color:#66d9ef">if</span> <span style="color:#a6e22e">n</span> <span style="color:#f92672">&lt;=</span> <span style="color:#a6e22e">MAX_SIZE</span> {
        <span style="color:#75715e">// crop
</span><span style="color:#75715e"></span>        <span style="color:#a6e22e">data</span> <span style="color:#f92672">:=</span> (<span style="color:#f92672">*</span><span style="color:#a6e22e">goArray</span>)[:<span style="color:#a6e22e">n</span>]
        <span style="color:#a6e22e">fmt</span>.<span style="color:#a6e22e">Println</span>(<span style="color:#a6e22e">data</span>)
    }
}
</code></pre></div><p>The problem is the need to set the size of the array. The trick is to use
a great size (<code>MAX_SIZE</code>) and then crop the array (it turns is into a slice). Why not directly cast to a slice? A pointer to a <code>Go</code> array is the address of its first element (like in <code>C</code>), but a slice is a structure (see <a href="https://blog.golang.org/slices-intro">https://blog.golang.org/slices-intro</a> and <a href="https://golang.org/src/runtime/slice.go">https://golang.org/src/runtime/slice.go</a>).</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#66d9ef">type</span> <span style="color:#a6e22e">slice</span> <span style="color:#66d9ef">struct</span> {
    <span style="color:#a6e22e">array</span> <span style="color:#a6e22e">unsafe</span>.<span style="color:#a6e22e">Pointer</span> <span style="color:#75715e">// pointer to the real buffer
</span><span style="color:#75715e"></span>    <span style="color:#a6e22e">len</span>   <span style="color:#66d9ef">int</span>            <span style="color:#75715e">// number of elements in buffer
</span><span style="color:#75715e"></span>    <span style="color:#a6e22e">cap</span>   <span style="color:#66d9ef">int</span>            <span style="color:#75715e">// total capacity of the buffer
</span><span style="color:#75715e"></span>}
</code></pre></div><p>Thus, a pointer to a slice is just a pointer to such a structure so the cast will not work (and we cannot create a slice by providing the pointer to its underlying buffer).</p>
<h2 id="last-few-elements">Last few elements</h2>
<h3 id="memory">Memory</h3>
<p>Let us talk a bit about memory. In the previous examples, we created numpy arrays and we sent their pointers to a shared library. Their memory is then allocated by <code>python</code>, so we must care about not letting other code free it.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c" data-lang="c"><span style="color:#66d9ef">void</span> <span style="color:#a6e22e">scalar_mul</span>(<span style="color:#66d9ef">double</span> <span style="color:#f92672">*</span>v, size_t n, <span style="color:#66d9ef">double</span> scalar)
{
    <span style="color:#66d9ef">for</span> (size_t i <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>; i <span style="color:#f92672">&lt;</span> n; i<span style="color:#f92672">++</span>)
    {
        v[i] <span style="color:#f92672">=</span> scalar <span style="color:#f92672">*</span> v[i];
    }
}
</code></pre></div><p>Indeed, if we add the function above to our shared library, we see that it would modify our array.
We can check it in our <code>python</code> code:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">library<span style="color:#f92672">.</span>scalar_mul<span style="color:#f92672">.</span>argtypes <span style="color:#f92672">=</span> [ND_POINTER_1, c_size_t, c_double]
library<span style="color:#f92672">.</span>scalar_mul<span style="color:#f92672">.</span>restype <span style="color:#f92672">=</span> None

<span style="color:#75715e"># X = [1 1 1 1 1]</span>
X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>ones(<span style="color:#ae81ff">5</span>)
library<span style="color:#f92672">.</span>scalar_mul(X, X<span style="color:#f92672">.</span>size, <span style="color:#ae81ff">3.0</span>)
<span style="color:#66d9ef">print</span>(X)
</code></pre></div><p>It basically outputs what we expect:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ python3 test.py
<span style="color:#f92672">[</span>3. 3. 3. 3. 3.<span style="color:#f92672">]</span>
</code></pre></div><p>If we try to free the pointer, something bad is very likely to happen. On my laptop, my terminal totally freezes for instance.</p>
<h3 id="tools-to-generate-wrappers">Tools to generate wrappers</h3>
<p>Here we have done everything by hand (but thanks to the nice features of <code>python</code>). If you have a bigger library you want to wrap, this job can be quickly laborious. Some tools exist to automatically generate these wrappers. We can mention <a href="https://www.nongnu.org/g-wrap/">G-Wrap</a> and <a href="http://www.swig.org/">SWIG</a>.</p>
<p>I used the latter, it works fine with <code>C</code> and seems now to work well with <code>C++</code> (modern features are more and more supported). The advantage is that you give the function you want to wrap and it generates the <code>python</code> code. However, it create a kind of &ldquo;standard&rdquo; wrapper which overloads substantially all the objects. In the case of <code>numpy</code> arrays, we also ought to write extra code to make it really working since <code>swig</code> is not aware of our needs within <code>python</code>.</p>
<p>Thus these tools may be relevant according to your needs but doing it manually remains a simple solution it would work in any case.</p>
]]></content>
        </item>
        
        <item>
            <title>Machine Learning in Nmap</title>
            <link>https://asiffer.github.io/posts/nmap-ml/</link>
            <pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate>
            
            <guid>https://asiffer.github.io/posts/nmap-ml/</guid>
            <description>Notes
 The original post has been published on the Amossys blog I present some pieces of codes which are originally commented by their authors. I keep authors&amp;rsquo; comments under the format /* comments */ while my own comments use the format // comments. I use the version 7.70SVN of nmap.   First discovery After installing a basic Raspbian on my RPi, I also decided to add nmap to the system.</description>
            <content type="html"><![CDATA[<hr>
<p><strong>Notes</strong></p>
<ul>
<li>The original post has been published on the <a href="https://blog.amossys.fr/nmap-ml.html">Amossys blog</a></li>
<li>I present some pieces of codes which are originally commented by their authors. I keep authors&rsquo; comments under the format <code>/* comments */</code> while my own comments use the format <code>// comments</code>.</li>
<li>I use the version <strong>7.70SVN</strong> of <code>nmap</code>.</li>
</ul>
<hr>
<h2 id="first-discovery">First discovery</h2>
<p>After installing a basic Raspbian on my RPi, I also decided to add <code>nmap</code> to the system. Not so hard&hellip; but I was quite amazed.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">$ sudo apt install nmap
...
The following additional packages will be installed:
  liblinear3 liblua5.3-0 ndiff python-bs4 python-html5lib python-lxml
  python-webencodings
...
</code></pre></div><p>The package manager demands other packages, especially lua and python stuff. Ok, why not, but why <code>liblinear3</code> ?!
Actually, <code>liblinear</code> is a library for linear classification. Hum&hellip; it looks weird, so I decided to go deeper!</p>
<h2 id="deep-learning">Deep learning</h2>
<p>Sorry for the pun, but there is not any neural network behind that.
Looking at the code of <code>liblinear</code>, available on <a href="https://github.com/cjlin1/liblinear">GitHub</a>, we basically learn the following:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-markdown" data-lang="markdown">LIBLINEAR is a simple package for solving large-scale regularized linear
classification and regression. It currently supports
<span style="color:#66d9ef">-</span> L2-regularized logistic regression/L2-loss support vector classification/L1-loss support vector classification
<span style="color:#66d9ef">-</span> L1-regularized L2-loss support vector classification/L1-regularized logistic regression
<span style="color:#66d9ef">-</span> L2-regularized L2-loss support vector regression/L1-loss support vector regression.
</code></pre></div><p>In a word, it can do three things:</p>
<ul>
<li>logistic regression</li>
<li>support vector classification</li>
<li>support vector regression</li>
</ul>
<p>Even if the first technique aimed to fit data to a model, it is mainly used to perform binary classification.
The code boils down to few files:</p>
<ul>
<li><code>linear.h</code></li>
<li><code>tron.h</code></li>
</ul>
<div class="info">
    The whole implementation is probably very efficient as <code>liblinear</code> won the ICML 2008 large-scale learning challenge and has been used to win the KDD Cup 2010 (see <a href="https://www.csie.ntu.edu.tw/~cjlin/liblinear/)">https://www.csie.ntu.edu.tw/~cjlin/liblinear/)</a>.
</div>
<h2 id="coming-back-to-nmap">Coming back to Nmap</h2>
<p>Now, the goal is to look for <code>liblinear</code> inside the <code>nmap</code> code (about 85Mb).</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">$ git clone https://github.com/nmap/nmap.git
</code></pre></div><p>The natural idea is to <em>grep</em> &ldquo;linear.h&rdquo; in the folder.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">$ cd nmap
$ grep -r linear.h  
liblinear/Makefile:linear.o: linear.cpp linear.h
liblinear/liblinear.vcxproj:    &lt;ClInclude Include<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;linear.h&#34;</span> /&gt;
liblinear/Makefile.win:linear.obj: linear.cpp linear.h
liblinear/Makefile.win:lib: linear.cpp linear.h linear.def tron.obj
liblinear/linear.cpp:#include <span style="color:#e6db74">&#34;linear.h&#34;</span>
liblinear/predict.c:#include <span style="color:#e6db74">&#34;linear.h&#34;</span>
liblinear/train.c:#include <span style="color:#e6db74">&#34;linear.h&#34;</span>
FPModel.cc:#include <span style="color:#e6db74">&#34;linear.h&#34;</span>
configure.ac:  AC_CHECK_HEADERS<span style="color:#f92672">([</span>linear.h<span style="color:#f92672">]</span>,
FPEngine.cc:#include <span style="color:#e6db74">&#34;linear.h&#34;</span>
configure:  <span style="color:#66d9ef">for</span> ac_header in linear.h
configure:  ac_fn_c_check_header_mongrel <span style="color:#e6db74">&#34;</span>$LINENO<span style="color:#e6db74">&#34;</span> <span style="color:#e6db74">&#34;linear.h&#34;</span> <span style="color:#e6db74">&#34;ac_cv_header_linear_h&#34;</span> <span style="color:#e6db74">&#34;</span>$ac_includes_default<span style="color:#e6db74">&#34;</span>
configure:if test <span style="color:#e6db74">&#34;x</span>$ac_cv_header_linear_h<span style="color:#e6db74">&#34;</span> <span style="color:#f92672">=</span> xyes; <span style="color:#66d9ef">then</span> :
</code></pre></div><p>The result shows a dozen of occurrences. The &ldquo;configure&rdquo; file is not interesting. The occurrences inside <code>liblinear/</code> are not relevant as <code>nmap</code> stores the library code inside it. Finally we have two source code files <code>FPModel.cc</code> and <code>FPEngine.cc</code> which draw our attention (&lsquo;FP&rsquo; means &lsquo;finger-printing&rsquo;).
In the folder we can also find two headers <code>FPModel.h</code> and <code>FPEngine.h</code>.</p>
<p>In <code>FPModel.h</code>, five objects are introduced.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cpp" data-lang="cpp"><span style="color:#66d9ef">extern</span> <span style="color:#66d9ef">struct</span> <span style="color:#a6e22e">model</span> FPModel;
<span style="color:#66d9ef">extern</span> <span style="color:#66d9ef">double</span> FPscale[][<span style="color:#ae81ff">2</span>];
<span style="color:#66d9ef">extern</span> <span style="color:#66d9ef">double</span> FPmean[][<span style="color:#ae81ff">695</span>];
<span style="color:#66d9ef">extern</span> <span style="color:#66d9ef">double</span> FPvariance[][<span style="color:#ae81ff">695</span>];
<span style="color:#66d9ef">extern</span> FingerMatch FPmatches[];
</code></pre></div><p>The <code>FPEngine.h</code> is quite richer and notably defines an <code>FPEngine</code> object</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cpp" data-lang="cpp"><span style="color:#75715e">/* This class is the generic fingerprinting engine. */</span>
<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">FPEngine</span> {

<span style="color:#66d9ef">protected</span><span style="color:#f92672">:</span>
  size_t osgroup_size;

<span style="color:#66d9ef">public</span><span style="color:#f92672">:</span>
  FPEngine();
  <span style="color:#f92672">~</span>FPEngine();
  <span style="color:#66d9ef">void</span> <span style="color:#a6e22e">reset</span>();
  <span style="color:#66d9ef">virtual</span> <span style="color:#66d9ef">int</span> <span style="color:#a6e22e">os_scan</span>(std<span style="color:#f92672">::</span>vector<span style="color:#f92672">&lt;</span>Target <span style="color:#f92672">*&gt;</span> <span style="color:#f92672">&amp;</span>Targets) <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>;
  <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">char</span> <span style="color:#f92672">*</span><span style="color:#a6e22e">bpf_filter</span>(std<span style="color:#f92672">::</span>vector<span style="color:#f92672">&lt;</span>Target <span style="color:#f92672">*&gt;</span> <span style="color:#f92672">&amp;</span>Targets);

};
</code></pre></div><p>Looking at the methods of <code>FPEngine</code>, we start to understand the purpose of <code>liblinear</code>: OS detection, which is a very nice and powerful feature in practice, isn&rsquo;t it?</p>
<h2 id="a-fingerprint-model">A fingerprint model</h2>
<p>While the header <code>FPModel.h</code> is rather small, the objects it declares are really heavy! They are all implemented in <code>FPModel.cc</code>.
In particular the <code>FPModel</code> structure is an instance of a <code>model</code> defined in <code>liblinear.h</code>:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cpp" data-lang="cpp"><span style="color:#66d9ef">struct</span> <span style="color:#a6e22e">model</span>
{
	<span style="color:#66d9ef">struct</span> <span style="color:#a6e22e">parameter</span> param;
	<span style="color:#66d9ef">int</span> nr_class;		<span style="color:#75715e">/* number of classes */</span>
	<span style="color:#66d9ef">int</span> nr_feature;
	<span style="color:#66d9ef">double</span> <span style="color:#f92672">*</span>w;
	<span style="color:#66d9ef">int</span> <span style="color:#f92672">*</span>label;		<span style="color:#75715e">/* label of each class */</span>
	<span style="color:#66d9ef">double</span> bias;
};
</code></pre></div><p>The FPModel has the following attribute values:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cpp" data-lang="cpp"><span style="color:#66d9ef">struct</span> <span style="color:#a6e22e">model</span> FPModel <span style="color:#f92672">=</span> {
	{<span style="color:#ae81ff">0</span>},			<span style="color:#75715e">// L2R_LR solver 
</span><span style="color:#75715e"></span>                    <span style="color:#75715e">// (L2-regularized classifiers 
</span><span style="color:#75715e"></span>                    <span style="color:#75715e">// and logistic regression)
</span><span style="color:#75715e"></span>	<span style="color:#ae81ff">96</span>, 			<span style="color:#75715e">// number of classes
</span><span style="color:#75715e"></span>	<span style="color:#ae81ff">695</span>,			<span style="color:#75715e">// number of features
</span><span style="color:#75715e"></span>	_w,			    <span style="color:#75715e">// weights (695 x 96 values)
</span><span style="color:#75715e"></span>	_labels,		<span style="color:#75715e">// index of the classes ([0, 1 ..., 95])
</span><span style="color:#75715e"></span>	<span style="color:#f92672">-</span><span style="color:#ae81ff">1.00000000</span> 	<span style="color:#75715e">// -1 means no bias
</span><span style="color:#75715e"></span>};
</code></pre></div><p><strong>What do we learn?</strong> Nmap embeds a classification task. For every observation, i.e. a vector of size 695 (695 features), the goal is to find the class that best matches it among the 96 available.</p>
<p><strong>What are the observations?</strong> Basically, they are the fingerprints. They are represented by 695 features.</p>
<p><strong>What are the features?</strong> The 695 values are the results of network scanning.</p>
<p><strong>What are the classes?</strong> The array <code>FPmatches</code> notably details the 96 classes: they are OS with specific version (for instance we have &ldquo;Linux 2.6.11 - 2.6.15&rdquo; and &ldquo;Linux 2.6.32 - 2.6.39&rdquo; but also &ldquo;Netgear DGN3300v2 ADSL router&rdquo;).</p>
<p>Currently, we don&rsquo;t know exactly how the classification is performed, but it will basically use the local variable <code>_w</code> which is a big weights array. In a word, we can say that the classification model is stored in <code>_w</code>.</p>
<p>Three structures have not been unveiled: <code>FPscale</code>, <code>FPmean</code> and <code>FPvariance</code>. Let us have a look to the first:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cpp" data-lang="cpp"><span style="color:#75715e">/* Scale parameters are pairs (a, b). A scaled value x&#39; is calculated from
</span><span style="color:#75715e">   a, b, and an observed x by x&#39; = (x + a) * b. */</span>
<span style="color:#66d9ef">double</span> FPscale[][<span style="color:#ae81ff">2</span>] <span style="color:#f92672">=</span> {
	{        <span style="color:#f92672">-</span><span style="color:#ae81ff">20</span>,  <span style="color:#ae81ff">0.0416667</span> },	<span style="color:#75715e">/* S1.PLEN */</span>
	{         <span style="color:#f92672">-</span><span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0.00520833</span> },	<span style="color:#75715e">/* S1.TC */</span>
	{        <span style="color:#f92672">-</span><span style="color:#ae81ff">64</span>,  <span style="color:#ae81ff">0.0052356</span> },	<span style="color:#75715e">/* S1.HLIM */</span>
	{        <span style="color:#f92672">-</span><span style="color:#ae81ff">20</span>,  <span style="color:#ae81ff">0.0416667</span> },	<span style="color:#75715e">/* S2.PLEN */</span>
	{         <span style="color:#f92672">-</span><span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0.00520833</span> },	<span style="color:#75715e">/* S2.TC */</span>
        ...
</code></pre></div><p>The structure is a 695x2 matrix. At each line, a comment describes a quantitative feature used for OS detection. The values inside a row aim to &ldquo;scale&rdquo; the value of the feature.</p>
<div class="info">
    When we want to classify data according to some features, it is often relevant to balance their impact. In particular, if a feature lies between 1000 and 10000, it is likely to carry more weight during the classifier training than a feature stuck between 0 and 1. Therefore, features are &ldquo;scaled&rdquo; to make them lie in the same range.
</div>
<p>Even if the use of these coefficients <code>a</code> and <code>b</code> are weird (I would rather use a mean and a standard deviation), they do the job (the author&rsquo;s comment details how features are scaled).</p>
<p><code>FPscale</code> normalizes the data, so&hellip; what is the purpose of <code>FPmean</code> and <code>FPvariance</code> ??! We will see that later.</p>
<h2 id="classifying-the-fingerprint">Classifying the fingerprint</h2>
<p>While <code>FPModel.cc</code> gathers data, detection procedures are implemented in <code>FPEngine.cc</code>. The function we are interested in is:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cpp" data-lang="cpp"><span style="color:#66d9ef">static</span> <span style="color:#66d9ef">void</span> classify(FingerPrintResultsIPv6 <span style="color:#f92672">*</span>FPR)
</code></pre></div><p>Here we see a specificity: <code>nmap</code> uses this ML process only to classify IPv6 fingerprints. For IPv4 fingerprints, it actually tests them against fingerprint references (you can find at <code>/usr/share/nmap/nmap-os-db</code>). More details can be found at <a href="https://nmap.org/book/osdetect-guess.html#osdetect-guess-ipv4">OS Matching Algorithm</a>.</p>
<p>The <code>classify</code> function takes a fingerprint as input and stores the basic classification result inside the <code>FPR-&gt;overall_results</code> attribute (<code>OSSCAN_NOMATCHES</code> or <code>OSSCAN_SUCCESS</code>). More elements are stored in other attributes like <code>FPR-&gt;matches</code> or <code>FPR-&gt;accuracy</code>. Let us describe how this function treats a fingerprint:</p>
<p>First, it retrieves the features from the fingerprint itself:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cpp" data-lang="cpp">features <span style="color:#f92672">=</span> vectorize(FPR);
</code></pre></div><p>Then features are scaled:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cpp" data-lang="cpp">apply_scale(features, get_nr_feature(<span style="color:#f92672">&amp;</span>FPModel), FPscale);
</code></pre></div><p>Finally some scores are computed by the algorithms inside <code>liblinear</code>. Every score estimates the similarity between the current fingerprint (<code>features</code>) and a given OS.
As these scores come from a logistic regression, they can be turned into probabilities.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cpp" data-lang="cpp">predict_values(<span style="color:#f92672">&amp;</span>FPModel, features, values);
  <span style="color:#66d9ef">for</span> (i <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>; i <span style="color:#f92672">&lt;</span> nr_class; i<span style="color:#f92672">++</span>) {
    labels[i].label <span style="color:#f92672">=</span> i;
    labels[i].prob <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span> <span style="color:#f92672">/</span> (<span style="color:#ae81ff">1.0</span> <span style="color:#f92672">+</span> exp(<span style="color:#f92672">-</span>values[i]));
  }
</code></pre></div><p>All these probabilities are then sorted in descending order.
The OS giving the highest one is considered as a <em>perfect match</em>. Moreover, all the OS getting a probability higher that 90% of the highest probability are also considered as perfect matches.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cpp" data-lang="cpp"><span style="color:#66d9ef">if</span> (labels[i].prob <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0.90</span> <span style="color:#f92672">*</span> labels[<span style="color:#ae81ff">0</span>].prob)
      FPR<span style="color:#f92672">-&gt;</span>num_perfect_matches <span style="color:#f92672">=</span> i <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>;
</code></pre></div><p>After checking all the scores, the code looks at the number of perfect matches. If there is not exactly one perfect match, <code>OSSCAN_NOMATCHES</code> is set (meaning that OS has not been detected). It is rather strange because if we have two or more matches, we unfortunately get no result.
However, this event could possibly not happen once classes are well separated.</p>
<h2 id="what-does-liblinear-do-exactly">What does liblinear do exactly?</h2>
<p>As we said <code>liblinear</code> provides several solvers to perform linear regression/classification. <code>nmap</code> uses a regularized logistic regression to classify fingerprints. Logistic regression is basically used for a 2-classes classification task, but it can be extended to greater number of classes with a <em>one-versus-all</em> approach (this is what <code>liblinear</code> does).</p>
<div class="info">
    Let us consider a classification problem with <code>k</code> classes. The one-versus-all strategy consists in building <code>k</code> binary classifiers. The binary classifier <code>i</code> tries to separate the observations belonging to the class <code>i</code> from all the other observations.
When a new observation has to be classified, each classifier can provide the probability <code>p_i</code> that it belongs to the class <code>i</code>. The class with maximum probability is naturally kept.*
</div>
<h2 id="best-does-not-mean-right">Best does not mean right</h2>
<p>The function <code>classify</code> actually goes deeper as it performs another check on the best found OS. Precisely, it calls the function <code>novelty_of</code> and check its output:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cpp" data-lang="cpp">novelty <span style="color:#f92672">=</span> novelty_of(features, labels[<span style="color:#ae81ff">0</span>].label);
...
<span style="color:#66d9ef">if</span> (novelty <span style="color:#f92672">&lt;</span> FP_NOVELTY_THRESHOLD) {
      FPR<span style="color:#f92672">-&gt;</span>overall_results <span style="color:#f92672">=</span> OSSCAN_SUCCESS;
    } <span style="color:#66d9ef">else</span> {
      ...
      FPR<span style="color:#f92672">-&gt;</span>overall_results <span style="color:#f92672">=</span> OSSCAN_NOMATCHES;
	  ...
    }
</code></pre></div><p><strong>What does this function do?</strong> It actually computes a distance between our observation (<code>features</code>) and the group of all the observations belonging the predicted class (given through <code>labels[0].label</code>).</p>
<p>For that purpose, it uses&hellip; <code>FPmean</code> and <code>FPVariance</code> structures! They are merely 96x695 matrices where <code>FPmean[i][j]</code> represents the mean of the feature <code>j</code> for the OS <code>i</code> (<code>FPvariance</code> gives the corresponding variance).</p>
<p>Actually, the computed distance looks like the <a href="https://en.wikipedia.org/wiki/Mahalanobis_distance">Mahalanobis distance</a> except that it does not take into account the correlation between features. In a way, it assumes that there is no linear relation between the features (a stronger assumption would be that they are indepedent): for sure this is a wrong assumption but it does not prevent it from working in practice.</p>
<div class="info">
    The code is well documented and authors give many details about this function. In particular, they explain they use this approximation to <em>save space</em>.
</div>
<p>As we see, it uses a harcoded threshold, defined in <code>FPEngine.h</code>:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cpp" data-lang="cpp"><span style="color:#75715e">#define FP_NOVELTY_THRESHOLD 15.0
</span></code></pre></div><p>Eventually, the whole approach is rather relevant: the fingerprint is compared to all the OS. The &ldquo;closest&rdquo; OS (the one with highest probability) is kept but we ensure that the fingerprint is not too far from other observations sharing this OS.</p>
<h2 id="conclusion">Conclusion</h2>
<p>OS detection is obviously a powerful feature of <code>nmap</code>, that is also why we could be curious to know how it works. While IPv4 fingerprints are basically tested against some references, a ML-based approach is used to classify IPv6 ones.</p>
<p>Forget about fancy deep stuff algorithms! <code>nmap</code> uses a logistic regression to estimate the probability of a fingerprint to belong to a certain class (OS). Furthermore, it computes a novelty score which aims to avoid misclassification.
The whole process is summed up below (for official details, you can look at <a href="https://nmap.org/book/osdetect-guess.html#osdetect-guess-ipv6">IPv6 Matching</a> )</p>
<p>
    <img src="/images/nmap/nmap.png"  class="left"  />


<div class="caption">
    Figure 1: IPv6 OS detection process in <code>nmap</code>
</div></p>
<p>Despite its practical efficiency, we can do a couple of remarks:</p>
<ul>
<li>The model (all the blue elements) is totally static, a bit crude (raw values) and hard to enrich by ourself.</li>
<li>Some aspects are more about hacking: uncommon pipeline, use of hardcoded constants and strange choices (about the number of matches especially).</li>
</ul>
<p>I personnally think that we could improve the whole detection engine (using also an ML approach for IPv4). The key elements would be to use a more flexible design and to set up a more &ldquo;modern&rdquo; detection workflow (to try it at least). I develop some of these ideas in the last paragraph.</p>
<h2 id="ideas-to-improve-nmap">Ideas to improve NMAP</h2>
<p>The built-in <code>nmap</code> model is very powerful as it takes into account almost hundred OS. However it is static and possibly updated only when a new version is available.</p>
<p>Imagine you have fingerprints of very uncommon systems like SCADA or seldom firmwares. You have to submit this fingerprint to update the whole model (updating the model locally is rather hard). You possibly don&rsquo;t want to share your fingerprints or even you are very motivated, it will take time as we can see in that <a href="https://superuser.com/a/1099650">post</a> of <a href="http://bonsaiviking.com/">Daniel Miller</a>:</p>
<blockquote>
<p>[&hellip;] integrating user-submitted fingerprints is a manual process that takes several weeks of dedicated time to accomplish each year. We (the Nmap developers) are always looking for ways to improve this process and make more frequent updates, but generally there are only 2 releases per year.</p>
</blockquote>
<p>So the idea would be to have a more dynamic model that every <code>nmap</code> user can enrich (interactively for instance) and possibly share. A solution would be to have a fingerprint database. Yes, it already exists: <code>/usr/share/nmap/nmap-os-db</code>, but this is for IPv4 fingerprints. Moreover fingerprints should be stored directly as a feature vector. Thus, the model could be easily rebuilt by the user.</p>
<p>Furthermore, the current process computes the novelty of a new fingerprint but this piece of information is not exploited either while it could improve our detection engine.</p>
<p><strong>Can we use a even more powerful workflow?</strong> It is hard to talk about the choice of the algorithms. The number of features is quite high (695), but even if <code>liblinear</code> is made for large scale datasets, I would rather have a look to other classifiers like <a href="https://en.wikipedia.org/wiki/Random_forest">Random Forest</a> which does not need scaled data, may select the most relevant features and fits all the classes at the same time (no one-versus-all approach). About novelty detection, the computed distance is a bit uncommon. From the random forest classifier, it is possible to set a kind of &ldquo;certainty threshold&rdquo; below which the fingerprint is not classified. Another way would be to compute the <a href="https://en.wikipedia.org/wiki/Local_outlier_factor">Local Outlier Factor</a> (LOF) to check if the fingerprint looks like its neighbours (but it requires several labeled samples).</p>
]]></content>
        </item>
        
        <item>
            <title>Fancy Threat Intel&#39; with Cowrie/MySQL/Grafana stack</title>
            <link>https://asiffer.github.io/posts/threat-intel/</link>
            <pubDate>Fri, 25 Jan 2019 00:00:00 +0000</pubDate>
            
            <guid>https://asiffer.github.io/posts/threat-intel/</guid>
            <description>We all dream of this world map with cyber attacks between countries like this one or the one of Kaspersky. In this post, we will try to do threat intelligence at our modest level. We will consider a basic system open on the Internet, in the grip of cyber-attackers. In this context, we will collect attacks information so as to make a fancy dashboard to visualize what is going on.</description>
            <content type="html"><![CDATA[<p>We all dream of this world map with cyber attacks between countries like this one or the one of <a href="https://cybermap.kaspersky.com/">Kaspersky</a>. In this post, we will try to do threat intelligence at our modest level. We will consider a basic system open on the Internet, in the grip of cyber-attackers. In this context, we will collect attacks information so as to make a fancy dashboard to visualize what is going on.</p>
<p>We will use Cowrie to draw attackers&rsquo; attention (ssh honeypot), MySQL to store the collected data and Grafana to build a dashboard.</p>

    <img src="https://digitalguardian.com/sites/default/files/47275200_m.jpg"  class="left"  style="width: 100%; margin: 1em 0;"  />


<h2 id="context">Context</h2>
<p>In this basic threat intel&rsquo; experiment, we assume having a headless system with a public IP (<code>SERVER_IP</code>) and a domain name (<code>DOMAIN_NAME</code>). We can connect to the server through ssh.</p>
<p>We will first install Cowrie, then MySQL and finally Grafana. Cowrie is a honeypot which will listen on <code>SERVER_IP:22</code>. MySQL will only listen on localhost while Grafana will listen on <code>SERVER_IP:30003</code>.</p>
<h2 id="cowrie">Cowrie</h2>
<p><a href="https://github.com/cowrie/cowrie">Cowrie</a> is a common ssh honeypot aimed to log connection attempts. In particular, we will focus on retrieving the user/password used and also the IP of the attacker.</p>
<p>Several tutorials exist to install cowrie. I have mainly been inspired by <a href="https://eval2a.wordpress.com/2017/12/04/honeypot-part-1-setting-up-cowrie-and-dionaea/">this one</a>.</p>
<h3 id="before-installing-cowrie">Before installing cowrie</h3>
<p>To be a nice system to attack, we have to ensure that our honeypot will listen on port 22. So, as you probably have a ssh server, be sure it is listening on a different port (check /etc/ssh/sshd_config).</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#a6e22e">...</span>
<span style="color:#75715e"># The strategy used for options in the default sshd_config shipped with</span>
<span style="color:#75715e"># OpenSSH is to specify options with their default value where</span>
<span style="color:#75715e"># possible, but leave them commented.  Uncommented options override the</span>
<span style="color:#75715e"># default value.</span>

<span style="color:#a6e22e">Port 10022</span>
<span style="color:#75715e">#AddressFamily any</span>
<span style="color:#75715e">#ListenAddress 0.0.0.0</span>
<span style="color:#75715e">#ListenAddress ::</span>
<span style="color:#a6e22e">...</span>
</code></pre></div><p>Some dependencies are required before installing cowrie. Sorry, it uses python2.7 which will be soon <a href="https://pythonclock.org/">deprecated</a>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ sudo apt install git python-virtualenv libssl-dev libffi-dev build-essential libpython-dev python2.7-minimal
</code></pre></div><p>Finally, we need to create the <code>cowrie</code> user. Its home folder will host the logs. However, in my case, I have not so much memory in /home, so I will put everything in another folder, namely /data. So cowrie&rsquo;s home will be at /data/cowrie.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ sudo adduser --home /data/cowrie --disabled-password cowrie
</code></pre></div><p>It may ask you some useless extra information about the user.</p>
<h3 id="getting-sources">Getting sources</h3>
<p>Now, let us take the <code>cowrie</code> identity to download the honeypot files.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ sudo su cowrie
$ cd /data/cowrie
$ git clone https://github.com/micheloosterhof/cowrie
</code></pre></div><p>Data are in /data/cowrie/cowrie folder. In this folder, we will create a virtual environment (to avoid installing further python packets on the system).</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ cd cowrie
$ virtualenv cowrie-env
Running virtualenv with interpreter /usr/bin/python2
New python executable in /data/cowrie/cowrie/cowrie-env/bin/python2
Also creating executable in /data/cowrie/cowrie/cowrie-env/bin/python
Installing setuptools, pkg_resources, pip, wheel...done.
</code></pre></div><p>Then, we enter in the virtual environment to download some requirements.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ source cowrie-env/bin/activate
<span style="color:#f92672">(</span>cowrie-env<span style="color:#f92672">)</span> $ pip install --upgrade pip
DEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won<span style="color:#960050;background-color:#1e0010">&#39;</span>t be maintained after that date. A future version of pip will drop support <span style="color:#66d9ef">for</span> Python 2.7.
<span style="color:#f92672">(</span>cowrie-env<span style="color:#f92672">)</span> $ pip install -r requirements.txt --upgrade
...
<span style="color:#f92672">(</span>cowrie-env<span style="color:#f92672">)</span> $ deactivate
</code></pre></div><h3 id="configure">Configure</h3>
<p>Let us recall that we are in the /data/cowrie/cowrie folder. The cowrie config is located in etc/ subfolder. Let us have a look.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ cd etc/
$ cp cowrie.cfg.dist cowrie.cfg
</code></pre></div><p>Many things can be changed in cowrie.cfg, but we will focus on the most important.</p>
<h4 id="listening-port">Listening port</h4>
<p>In the cowrie.cfg we have especially the port cowrie will listen.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#75715e"># Endpoint to listen on for incoming SSH connections.</span>
<span style="color:#a6e22e">listen_endpoints</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">tcp:2222:interface=0.0.0.0</span>
</code></pre></div><p>So cowrie is going to listen on tcp port 2222. This is not very interesting to make our system scanned by bots. Several solutions exist.</p>
<h5 id="not-recommended">Not recommended</h5>
<p>Change 2222 to 22 in the config file and run cowrie as root (no, don&rsquo;t do that).</p>
<h5 id="add-bind-capability">Add bind capability</h5>
<p>Change 2222 to 22 in the config file and add bind capabilities to python executable (linux capability <code>CAP_NET_BIND_SERVICE</code>). As your classical user:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ sudo setcap cap_net_bind_service<span style="color:#f92672">=</span>+ep /data/cowrie/cowrie/cowrie-env/bin/python2
</code></pre></div><h5 id="authbind">Authbind</h5>
<p>(see <a href="https://eval2a.wordpress.com/2017/12/04/honeypot-part-1-setting-up-cowrie-and-dionaea/">here</a>)</p>
<h5 id="iptables">iptables</h5>
<p>Finally, you can redirect the port 22 with iptables. As your classical user:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ sudo iptables -t nat -A PREROUTING -p tcp –dport <span style="color:#ae81ff">22</span> -j REDIRECT –to-port <span style="color:#ae81ff">2222</span>
</code></pre></div><p>I have tried iptables and capabilites methods. It works well in both cases.</p>
<h4 id="accepted-connections">Accepted connections</h4>
<p>Cowrie does just log connection but also event about what the attackers do if he manages to connect. WTF!? Don&rsquo;t worry, it is a kind of sandbox. Nowadays, you can configure the accepted credentials through the etc/userdb.txt file (copy userdb.example to userdb.txt before). In my case I have not given any access.</p>
<h3 id="start-cowrie">Start cowrie</h3>
<p>Finally you can start cowrie (the first line is to change the user to cowrie):</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ sudo su cowrie
$ cd /data/cowrie/cowrie
$ ./bin/cowrie start

Join the Cowrie community at: http://bit.ly/cowrieslack

Using default Python virtual environment <span style="color:#e6db74">&#34;/data/cowrie/cowrie/cowrie-env&#34;</span>
Starting cowrie: <span style="color:#f92672">[</span>twistd   --umask<span style="color:#f92672">=</span><span style="color:#ae81ff">0022</span> --pidfile<span style="color:#f92672">=</span>var/run/cowrie.pid --logger cowrie.python.logfile.logger cowrie <span style="color:#f92672">]</span>...
</code></pre></div><p>Eventually you can check it works.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ ./bin/cowrie status
cowrie is running <span style="color:#f92672">(</span>PID: 19133<span style="color:#f92672">)</span>.
$ ss -alt
State  Recv-Q Send-Q Local Address:Port Peer Address:Port
LISTEN <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">50</span>           0.0.0.0:ssh       0.0.0.0:*   
</code></pre></div><p>For the moment you can stop cowrie. We will have a look on some of its options later.</p>
<h3 id="caas-cowrie-as-a-service">CAAS: Cowrie As A Service</h3>
<p>To my mind, managing cowrie is not user-friendly enough. We have to log as the <code>cowrie</code> user and launch <code>/data/cowrie/cowrie/bin/cowrie start</code>. Why not using a systemd service file? Let us create the file <code>cowrie.service</code> in the folder <code>/usr/lib/systemd/system/</code>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#66d9ef">[Unit]</span>
<span style="color:#a6e22e">Description</span><span style="color:#f92672">=</span><span style="color:#e6db74">COWRIE: the famous ssh honeypot!</span>
<span style="color:#a6e22e">Requires</span><span style="color:#f92672">=</span><span style="color:#e6db74">mysql.service</span>

<span style="color:#66d9ef">[Service]</span>
<span style="color:#a6e22e">User</span><span style="color:#f92672">=</span><span style="color:#e6db74">cowrie</span>
<span style="color:#a6e22e">ExecStart</span><span style="color:#f92672">=</span><span style="color:#e6db74">/data/cowrie/cowrie/bin/cowrie start</span>
<span style="color:#a6e22e">Restart</span><span style="color:#f92672">=</span><span style="color:#e6db74">on-failure</span>

<span style="color:#66d9ef">[Install]</span>
<span style="color:#a6e22e">WantedBy</span><span style="color:#f92672">=</span><span style="color:#e6db74">multi-user.target</span>
</code></pre></div><p>Now we just have to do:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">$ sudo systemctl daemon-reaload
$ sudo systemctl start cowrie
</code></pre></div><p>If you check the service (<code>sudo systemctl status cowrie</code>), it is actually idle&hellip; Why? The file <code>/data/cowrie/cowrie/bin/cowrie</code> is in fact a shell script which manage and daemonize the real underlying program. To solve the problem, we need to change a variable at the beginning of the file:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#a6e22e">...</span>
<span style="color:#a6e22e">DAEMONIZE</span><span style="color:#f92672">=</span><span style="color:#e6db74">&#34;-n&#34;</span>
<span style="color:#a6e22e">...</span>
</code></pre></div><p>Now you can start your service (even enable it at boot time)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">$ sudo systemctl start cowrie
$ sudo systemctl enable cowrie
</code></pre></div><h2 id="mysql">MySQL</h2>
<p>We will use MySQL to store the cowrie logs. First we set up a MySQL server and then we configure cowrie to send logs to the database.
Many of these steps are detailed in /data/cowrie/cowrie/docs/sql/README.rst.</p>
<h3 id="requirements">Requirements</h3>
<p>First let us download the mysql utilities.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ sudo apt install mysql-server mysql-client mysql-common libmysqlclient-dev
</code></pre></div><h3 id="configuration">Configuration</h3>
<p>First you can start the secure installation of mysql. It helps to remove some useless (and insecure) features.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ sudo mysql_secure_installation 
</code></pre></div><p>Then you can add a cowrie user, a database just for him (and all privilege on it).</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ sudo mysql
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql">mysql<span style="color:#f92672">&gt;</span> <span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">USER</span> <span style="color:#e6db74">&#39;cowrie&#39;</span><span style="color:#f92672">@</span><span style="color:#e6db74">&#39;localhost&#39;</span> IDENTIFIED <span style="color:#66d9ef">BY</span> <span style="color:#e6db74">&#39;cowrie&#39;</span>;
mysql<span style="color:#f92672">&gt;</span> <span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">DATABASE</span> cowrie;
mysql<span style="color:#f92672">&gt;</span> <span style="color:#66d9ef">GRANT</span> <span style="color:#66d9ef">ALL</span> <span style="color:#66d9ef">PRIVILEGES</span> <span style="color:#66d9ef">ON</span> cowrie.<span style="color:#f92672">*</span> <span style="color:#66d9ef">TO</span> <span style="color:#e6db74">&#39;cowrie&#39;</span><span style="color:#f92672">@</span><span style="color:#e6db74">&#39;localhost&#39;</span>;
mysql<span style="color:#f92672">&gt;</span> FLUSH <span style="color:#66d9ef">PRIVILEGES</span>;
</code></pre></div><p>After you can check that the connection with the cowrie&rsquo;s account is ok</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ mysql -u cowrie -p
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or <span style="color:#ae81ff">\g</span>.
...
</code></pre></div><h3 id="back-to-cowrie">Back to cowrie</h3>
<p>Cowrie can send logs to a mysql database. First we have to add a python package (to the virtual environment obviously):</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ sudo su cowrie
$ cd /data/cowrie/cowrie
$ source cowrie-env/bin/activate
<span style="color:#f92672">(</span>cowrie-env<span style="color:#f92672">)</span> $ pip install mysql-python --upgrade
</code></pre></div><p>Then, the config file of cowrie (/data/cowrie/cowrie/etc/cowrie.cfg) must be modified as follows (according to your MySQL installation).</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#a6e22e">...</span>
<span style="color:#75715e"># MySQL logging module</span>
<span style="color:#75715e"># Database structure for this module is supplied in docs/sql/mysql.sql</span>
<span style="color:#75715e"># </span>
<span style="color:#75715e"># MySQL logging requires extra software: sudo apt-get install libmysqlclient-dev</span>
<span style="color:#75715e"># MySQL logging requires an extra Python module: pip install mysql-python</span>
<span style="color:#75715e"># </span>
<span style="color:#66d9ef">[output_mysql]</span>
<span style="color:#a6e22e">enabled</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">true  </span>
<span style="color:#a6e22e">host</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">localhost </span>
<span style="color:#a6e22e">database</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">cowrie</span>
<span style="color:#a6e22e">username</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">cowrie</span>
<span style="color:#a6e22e">password</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">cowrie</span>
<span style="color:#a6e22e">port</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">3306</span>
<span style="color:#a6e22e">debug</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">false</span>
<span style="color:#a6e22e">...</span>
</code></pre></div><p>Now, the database scheme required by cowrie must be loaded in MySQL:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ cd /data/cowrie/cowrie/docs/sql
$ mysql -u cowrie -p
Enter password: 
...
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql">mysql<span style="color:#f92672">&gt;</span> USE cowrie;
mysql<span style="color:#f92672">&gt;</span> <span style="color:#66d9ef">source</span> mysql.<span style="color:#66d9ef">sql</span>;
mysql<span style="color:#f92672">&gt;</span> <span style="color:#66d9ef">show</span> tables;
<span style="color:#f92672">+</span><span style="color:#75715e">------------------+
</span><span style="color:#75715e"></span><span style="color:#f92672">|</span> Tables_in_cowrie <span style="color:#f92672">|</span>
<span style="color:#f92672">+</span><span style="color:#75715e">------------------+
</span><span style="color:#75715e"></span><span style="color:#f92672">|</span> auth             <span style="color:#f92672">|</span>
<span style="color:#f92672">|</span> clients          <span style="color:#f92672">|</span>
<span style="color:#f92672">|</span> downloads        <span style="color:#f92672">|</span>
<span style="color:#f92672">|</span> <span style="color:#66d9ef">input</span>            <span style="color:#f92672">|</span>
<span style="color:#f92672">|</span> keyfingerprints  <span style="color:#f92672">|</span>
<span style="color:#f92672">|</span> params           <span style="color:#f92672">|</span>
<span style="color:#f92672">|</span> sensors          <span style="color:#f92672">|</span>
<span style="color:#f92672">|</span> sessions         <span style="color:#f92672">|</span>
<span style="color:#f92672">|</span> ttylog           <span style="color:#f92672">|</span>
<span style="color:#f92672">+</span><span style="color:#75715e">------------------+
</span><span style="color:#75715e"></span>mysql<span style="color:#f92672">&gt;</span> EXIT
</code></pre></div><h2 id="grafana">Grafana</h2>
<h3 id="installation">Installation</h3>
<p>Some details can be found on the <a href="https://docs.grafana.org/installation/debian/">grafana website</a>. Unfortunately, you cannot use <code>apt</code> directly to get it, so you have either to download it manually or add their apt repository (see below).</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ sudo nano /etc/apt/sources.list.d/grafana.list
</code></pre></div><p>Add the following line:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-txt" data-lang="txt">deb https://packages.grafana.com/oss/deb stable main
</code></pre></div><p>To install singed packages:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ curl https://packages.grafana.com/gpg.key | sudo apt-key add -
</code></pre></div><p>Then install it!</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ sudo apt-get update
$ sudo apt-get install grafana
</code></pre></div><h3 id="configuration-https">Configuration (HTTPS)</h3>
<p>Before doing fancy visualizations with Grafana, we need to set up a bit of security. Actually, everything is local: cowrie listens on port 22 but this is a sandbox (besides we do not accept any connection in this case) and MySQL is listening on localhost.</p>
<p>Let us recall that we are headless so Grafana is likely to be our single entry point from the internet. As it has a full HTTP API, we first set up HTTPS, and then we create fancy dashboards.</p>
<p>Naturally, we will use <a href="https://certbot.eff.org/lets-encrypt/ubuntubionic-other">Certbot</a> to generate SSL certificates. We can add the cerbot ppa on our system so as to install it.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ sudo add-apt-repository ppa:certbot/certbot
$ sudo apt update
$ sudo apt install certbot 
</code></pre></div><p>With certbot we can easily create a certificate for our <code>DOMAIN_NAME</code>:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ sudo certbot certonly --standalone -d DOMAIN_NAME
...
IMPORTANT NOTES:
 - Congratulations! Your certificate and chain have been saved at:
   /etc/letsencrypt/live/DOMAIN_NAME/fullchain.pem
   Your key file has been saved at:
   /etc/letsencrypt/live/DOMAIN_NAME/privkey.pem
   Your cert will expire on 2019-04-28. To obtain a new or tweaked
   version of this certificate in the future, simply run certbot
   again. To non-interactively renew *all* of your certificates, run
   <span style="color:#e6db74">&#34;certbot renew&#34;</span>
...
</code></pre></div><p>We have the materials to make the connection to Grafana encryted. Let us open the configuration file (/etc/grafana/grafana.ini).
In particular we have to set the protocol to https, the port to 30003, the domain name and the certificate/key files.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#a6e22e">...</span>
<span style="color:#66d9ef">[server]</span>
<span style="color:#75715e"># Protocol (http, https, socket)</span>
<span style="color:#a6e22e">protocol</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">https</span>

<span style="color:#75715e"># The ip address to bind to, empty will bind to all interfaces</span>
<span style="color:#75715e">;http_addr =</span>

<span style="color:#75715e"># The http port  to use</span>
<span style="color:#a6e22e">http_port</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">30003</span>

<span style="color:#75715e"># The public facing domain name used to access grafana from a browser</span>
<span style="color:#a6e22e">domain</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">DOMAIN_NAME</span>

<span style="color:#75715e"># Redirect to correct domain if host header does not match domain</span>
<span style="color:#75715e"># Prevents DNS rebinding attacks</span>
<span style="color:#75715e">;enforce_domain = false</span>

<span style="color:#75715e"># The full public facing url you use in browser, used for redirects and emails</span>
<span style="color:#75715e"># If you use reverse proxy and sub path specify full url (with sub path)</span>
<span style="color:#75715e">;root_url = http://localhost:3000</span>
<span style="color:#a6e22e">...</span>
<span style="color:#75715e"># https certs &amp; key file</span>
<span style="color:#a6e22e">cert_file</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">/etc/letsencrypt/live/DOMAIN_NAME/cert.pem</span>
<span style="color:#a6e22e">cert_key</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">/etc/letsencrypt/live/DOMAIN_NAME/privkey.pem</span>
<span style="color:#a6e22e">...</span>
</code></pre></div><p>If you start your server, you may have an error: grafana cannot read your certificate files (cert/key). Actually, only root can read them while the Grafana service (file /usr/lib/systemd/system/grafana-server.service) run as <code>grafana</code> user (and <code>grafana</code> group). Two solutions exist (see the following paragraphs)</p>
<h5 id="lazy-and-ugly">Lazy and ugly</h5>
<p>You give read + exec access to all for the folders <code>/etc/letsencrypt/live</code> and <code>/etc/letsencrypt/archive</code> (the most insecure)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ sudo chmod -R a+rw /etc/letsencrypt/live
$ sudo chmod -R a+rw /etc/letsencrypt/archive
</code></pre></div><h5 id="boring-and-secure">Boring and secure</h5>
<p>You create a specific group for ssl stuff (e.g. <code>ssl-users</code>) and add grafana to it.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ sudo groupadd ssl-users
$ sudo usermod -aG ssl-users grafana
$ sudo chown -R root:ssl-users /etc/letsencrypt
$ sudo chmod -R g+rw /etc/letsencrypt/live
$ sudo chmod -R g+rw /etc/letsencrypt/archive
</code></pre></div><p>With the second solution, the problem is not solved yet. Actually, the service runs with the group <code>grafana</code> (and not <code>ssl-users</code>), so we have to remove it (in /usr/lib/systemd/system/grafana-server.service):</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#66d9ef">[Unit]</span>
<span style="color:#a6e22e">Description</span><span style="color:#f92672">=</span><span style="color:#e6db74">Grafana instance</span>
<span style="color:#a6e22e">Documentation</span><span style="color:#f92672">=</span><span style="color:#e6db74">http://docs.grafana.org</span>
<span style="color:#a6e22e">Wants</span><span style="color:#f92672">=</span><span style="color:#e6db74">network-online.target</span>
<span style="color:#a6e22e">After</span><span style="color:#f92672">=</span><span style="color:#e6db74">network-online.target</span>
<span style="color:#a6e22e">After</span><span style="color:#f92672">=</span><span style="color:#e6db74">postgresql.service mariadb.service mysql.service</span>

<span style="color:#66d9ef">[Service]</span>
<span style="color:#a6e22e">EnvironmentFile</span><span style="color:#f92672">=</span><span style="color:#e6db74">/etc/default/grafana-server</span>
<span style="color:#a6e22e">User</span><span style="color:#f92672">=</span><span style="color:#e6db74">grafana</span>
<span style="color:#75715e">#Group=grafana</span>
<span style="color:#a6e22e">Type</span><span style="color:#f92672">=</span><span style="color:#e6db74">simple</span>
<span style="color:#a6e22e">...</span>
</code></pre></div><p>When a service file is changed, we must reload them:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ sudo systemctl daemon-reload
</code></pre></div><p>Damned! It creates a new problem: <code>/etc/grafana</code> is owned by root:grafana, so the daemon cannot read the config file&hellip; You must change to grafana:grafana:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ sudo chown -R grafana:grafana /etc/grafana
</code></pre></div><p>We are done, you can check the status of the service:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ sudo service grafana-server status 
● grafana-server.service - Grafana instance
    Loaded: loaded <span style="color:#f92672">(</span>/usr/lib/systemd/system/grafana-server.service; disabled vendor preset: enabled<span style="color:#f92672">)</span>
    Active: active <span style="color:#f92672">(</span>running<span style="color:#f92672">)</span> since Mon 2019-01-28 12:43:54 UTC; 15min ago
    ...
</code></pre></div><h3 id="administration">Administration</h3>
<p>Finally! We did it! Let us connect to <code>https://DOMAIN_NAME:30003/</code>:

    <img src="/images/threat-intel/grafana_welcome.png"  class="left"  style="width: 100%; margin-bottom: 1em;"  />

</p>
<p>Now, you can connect to Grafana. Default credentials are admin/admin, but Grafana ask you to change once logged in, so&hellip; DO IT!</p>
<p>Then, on the main dashboard, Grafana invites us to add a data source. So, let&rsquo;s do this:</p>
<table>
<thead>
<tr>
<th>Home dashboard</th>
<th>Data source selection</th>
</tr>
</thead>
<tbody>
<tr>
<td>
    <img src="/images/threat-intel/grafana_home.png"  class="left"  style="width: 100%; margin-bottom: 1em;"  />

</td>
<td>
    <img src="/images/threat-intel/grafana_data_source.png"  class="left"  style="width: 100%; margin-bottom: 1em;"  />

</td>
</tr>
</tbody>
</table>
<p>Naturally, we select MySQL and we fill the required information:

    <img src="/images/threat-intel/grafana_mysql.png"  class="left"  style="width: 100%; margin-bottom: 1em;"  />

</p>
<p>After that, we are ready to make a fancy dashboard with fancy panels to visualize what is going on in our cowrie honeypot!</p>
<h3 id="first-panels">First panels</h3>
<p>Grafana have different built-in panels but you can get more through <a href="https://grafana.com/plugins">plugins</a>. In particular we will use the &ldquo;carpet-plot&rdquo;, which will represent the number of ssh connection attempts, every hour, along time. We can download it through:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ sudo grafana-cli plugins install petrslavotinek-carpetplot-panel
</code></pre></div><p>Once the plugin is installed, you must restart Grafana.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ sudo service grafana-server restart
</code></pre></div><h4 id="last-credentials">Last credentials</h4>
<p>One interesting thing is to get the last credentials the attackers used to connect to our honey pot. A simple &ldquo;Table&rdquo; panel can do the job.</p>
<p>To configure the panel, you have to make the right SQL query. Here, this is not so complicated as these information are stored in the <code>auth</code> table of the <code>cowrie</code> database. You can either use the query interface or put the raw query (<code>Toggle Edit Mode</code>).</p>
<p>In my point of view, the interface is better to manage time queries/aggregations. If you want simple output (no time filter), the edit mode is easier. So let us use it:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#66d9ef">SELECT</span> <span style="color:#66d9ef">timestamp</span>,username,password <span style="color:#66d9ef">from</span> auth
</code></pre></div>
    <img src="/images/threat-intel/grafana_panel_cred2.png"  class="left"  style="width: 100%; margin: 1em 0;"  />


<p>Now, you can configure further things like the title or the way to sort the data (descending time order in my case).</p>
<h4 id="number-of-unique-ip-addresses">Number of unique IP addresses</h4>
<p>The first panel is greedy, meaning that it has printed all the credentials since the beginning. However, Grafana is designed to work on sliding window. Here, we answer the following question: &ldquo;How many distinct IP addresses have attempted to connect for the last <code>x</code> days?&rdquo;, where <code>x</code> is a parameter you can tune on the top-right corner of the dashboard.</p>
<p>For that purpose, let us add a new &ldquo;Singlestat&rdquo; panel, designed to print a relevant value.

    <img src="/images/threat-intel/grafana_panel_distinct_ip.png"  class="left"  style="width: 100%; margin-bottom: 1em;"  />

</p>
<p>IP are stored in the <code>sessions</code> table.
Here, we have to count the distinct IP but only for the last <code>x</code> days. This filter is easily available through <code>$__timeFilter</code>. The whole command is then:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#66d9ef">SELECT</span> <span style="color:#66d9ef">COUNT</span>(<span style="color:#66d9ef">DISTINCT</span> ip) <span style="color:#66d9ef">FROM</span> sessions <span style="color:#66d9ef">WHERE</span> <span style="color:#960050;background-color:#1e0010">$</span>__timeFilter(starttime)
</code></pre></div><h4 id="attacking-periods">Attacking periods</h4>
<p>What time are we under attack? To go further in our threat intelligence experiment, we are going to use the &ldquo;carpet plot&rdquo; we downloaded previously.</p>
<p>Here, the query interface seems easier to use, especially if (like me) you are not mastering SQL.

    <img src="/images/threat-intel/grafana_panel_carpet3.png"  class="left"  style="width: 100%; margin-bottom: 1em;"  />

</p>
<p>Naturally, you can change the title, the color palette etc. Grafana and its plugins are pretty good for customization.</p>
<h4 id="i-want-the-world-map">I want the world map!</h4>
<p>Obsiously, we want the map. Unfortunately, the Grafana plugin is not so easy to use with our information. I will do my best to find
a simple solution to do it from our setup.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
]]></content>
        </item>
        
        <item>
            <title>Distribute your work with Git and Launchpad</title>
            <link>https://asiffer.github.io/posts/launchpad/</link>
            <pubDate>Tue, 07 Aug 2018 00:00:00 +0000</pubDate>
            
            <guid>https://asiffer.github.io/posts/launchpad/</guid>
            <description>Introduction Let us consider you have developped a nice tool (or a nice library). You make it work on your laptop but, more than that, you put all your code on GitHub of GitLab, for instance. As you are smart, you made a useful Makefile so as to help others to use your work through the famous process:
git clone https://remote.site/mytool.git cd mytool/ make make install However, it looks a bit handmade: of course, you don&amp;rsquo;t need to be a computer science expert to enter these commands, but this is not as easy as installing an app on your smartphone.</description>
            <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Let us consider you have developped a nice tool (or a nice library). You make it work on your laptop but, more than that, you put all your code on <a href="https://github.com/">GitHub</a> of <a href="https://about.gitlab.com/">GitLab</a>, for instance. As you are smart, you made a useful <code>Makefile</code> so as to help others to use your work through the famous process:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">git clone https://remote.site/mytool.git
cd mytool/
make
make install
</code></pre></div><p>However, it looks a bit handmade: of course, you don&rsquo;t need to be a computer science expert to enter these commands, but this is not as easy as installing an app on your smartphone. Moreover, for all the linux users which can use your work, this is not handled by their package manager: just think about the uninstall or update process &hellip;</p>
<p>Obviously, many ways exist to solve these drawbacks. Here we want to explain how you can distribute your work through debian packages and make it available for ubuntu user with a personal package archive (you know the <code>ppa:/...</code> you can add to your <code>/etc/apt/sources.list</code>).</p>
<h2 id="step-1-creating-a-debian-package">Step 1: Creating a debian package</h2>
<h3 id="makefile">Makefile</h3>
<p>First we have to ensure one thing: your <code>Makefile</code> must use an variable <code>DESTDIR</code> in the installation step. This variable will be used at the package creation. As an example, if you want to install your executable in <code>/usr/bin</code>, your <code>Makefile</code> can look like this:</p>
<pre><code>DESTDIR=
default: mytool

mytool:
    ...

install:
    install bin/mytool $(DESTDIR)/usr/bin

clean:
    rm -f bin/mytool

</code></pre><h3 id="skeleton">Skeleton</h3>
<p>Here we consider, we are in your git local repository. It may look like this:</p>
<pre><code>mytool/
    | bin/
    | build/
    | include/
    | src/
    | test/
    | Makefile
</code></pre><p>To create a debian package, we just have to add a <code>debian/</code> folder (with some specific files) in this layout. We can use <code>dh_make</code> to do it from scratch (in <code>mytool/ </code>):</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">dh_make -p mytool_1.0 --createorig
</code></pre></div><p>Then the type of the package is requeted:</p>
<pre><code>Type of package: (single, indep, library, python)
[s/i/l/p]?
</code></pre><p>If we want to distribute an executable, we just have to select <code>s</code>. After that, other details are asked:</p>
<pre><code>Maintainer Name     : YOU
Email-Address       : YOU@SOMEWHERE
Date                : Tue, 07 Aug 2018 10:00:00 +0200
Package Name        : mytool
Version             : 1.0
License             : blank
Package Type        : single
Are the details correct? [Y/n/q]
</code></pre><p>These details may depend on your linux configuration. So you can initially accept them. Henceforth, your layout is the following:</p>
<pre><code>| mytool_1.0.orig.tar.xz
| mytool/
    | bin/
    | build/
    | debian/
    | include/
    | src/
    | test/
    | Makefile
</code></pre><h3 id="editing-debian-files">Editing debian/* files</h3>
<p>Now, we have to edit files in the <code>debian/</code> folder. First there are a lot of examples that we can remove</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cd debian/
rm *.ex *.EX
</code></pre></div><p>Without too much details we have to ensure that the file <code>source/format</code> contains &ldquo;<strong>3.0 (native)</strong>&rdquo; (and not &ldquo;3.0 (quilt)&quot;). The <em>quilt</em> format use sucessive patches to modify your code from the original <code>mytool_1.0.orig.tar.xz</code> to your current version. Here we prefer build the package independently (without commiting the package changes etc.), in a kind of snapshot way.</p>
<p>I have also mentionned your personal details in the package initialization. The <code>control</code> file gatheres these information but also those about the package itself. It look like this:</p>
<pre><code>Source: mytool
Section: unknown
Priority: optional
Maintainer: YOU &lt;YOU@SOMEWHERE&gt;
Build-Depends: debhelper (&gt;= 10)
Standards-Version: 4.1.2
Homepage: &lt;insert the upstream URL, if relevant&gt;

Package: mytool
Architecture: any
Depends: ${shlibs:Depends}, ${misc:Depends}
Description: &lt;insert up to 60 chars description&gt;
 &lt;insert long description, indented with spaces&gt;
</code></pre><p>First you can edit the <strong>Maintainer</strong>, <strong>Homepage</strong> and <strong>Description</strong> sections. Then it could be nice to put your work into a common <a href="https://www.debian.org/doc/debian-policy/ch-archive.html#s-subsections">Section</a> (<strong>cli-mono</strong> for instance).</p>
<p>We can also notice the <strong>Build-Depends</strong> and <strong>Depends</strong> sections. In the former, you can precise if you need other packages to build your tool. For instance if you use a library (included in a debian package <code>libxxx-dev</code>), you can add it:</p>
<pre><code>...
Build-Depends: debhelper (&gt;= 10), libxxx-dev
...
</code></pre><p>The version can be precised in brackets. In the same way, the <strong>Depends</strong> section gives the dependencies to make your tool run:</p>
<pre><code>...
Depends: ${shlibs:Depends}, ${misc:Depends}, libxxx
...
</code></pre><p>In this latter section, we notice the variables <code>shlibs:Depends</code> and <code>misc:Depends</code>. Actually we can use them instead of adding manually  <code>libxxx</code> (they are defined in the <code>mytool.substvars</code> file).</p>
<h3 id="first-package">First package</h3>
<p>Once you have modifed the previous files, you are able to build your first package. First you have to add and commit your change with <code>git</code></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">git add debian/
git commit -m <span style="color:#e6db74">&#34;Towards a debian package&#34;</span>
</code></pre></div><p>Then we use <code>gbp</code> command (from the <code>git-buildpackage</code> package) to create the debian package. In <code>mytool/</code> folder:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">gbp buildpackage --git-ignore-new
</code></pre></div><p>The <code>git-ignore-new</code> options means that we want to build the package even if some changes have not been commited.
Then you package is ok. But, where is it ? In the parent folder! Your final layout may look like this:</p>
<pre><code>| mytool_1.0.orig.tar.xz
| mytool_1.0-1_amd64.buildinfo
| mytool_1.0-1_amd64.changes
| mytool_1.0-1_amd64.deb
| mytool_1.0-1.dsc
| mytool_1.0-1.tar.xz
| mytool_1.0.orig.tar.xz
mytool/
    | bin/
    | build/
    | debian/
    | include/
    | src/
    | test/
    | Makefile
</code></pre><h2 id="step-2-launchpad">Step 2: Launchpad</h2>
<p>Here we present how to use launchpad to perform automatic ubunutu builds from your code.</p>
<h3 id="a-new-remote-git-repository">A new remote git repository</h3>
<p>Launchpad allows you to create a Personal Package Archive (PPA). You can then distribute softwares and updates directly to Ubuntu users.</p>
<p>To log in to launchpad, you have to create a <a href="https://login.ubuntu.com/">Ubuntu One</a> account. Then you can attach your ssh key
(<a href="https://launchpad.net/~USER/+editsshkeys,">https://launchpad.net/~USER/+editsshkeys,</a> where USER is your username).
To host your future debian package, you have to create a new PPA (from your launchpad board).</p>
<p>Launchpad will host a remote git repository (the same you have in GitHub or GitLab). So, the idea would be to push your local commit to all your remote repo.
To make it easier, launchpad gives the following advice:
edit <code>~/.gitconfig</code> and add these lines, where USER is your username:</p>
<pre><code>[url &quot;git+ssh://USER@git.launchpad.net/&quot;]
        insteadof = lp:
</code></pre><p>Now you can add a new repo (called &ldquo;launchpad&rdquo;, but you can change it as you like)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">git remote add launchpad lp:~USER/+git/REPOSITORY-NAME
</code></pre></div><p>where REPOSITORY-NAME is naturally the name you want to give to the repo (generally the same as the others, so &ldquo;mytool&rdquo;).</p>
<p>Finally you can push your commits:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">git push origin master
git push launchpad master
</code></pre></div><h3 id="recipe">Recipe</h3>
<p>Why using another remote repo?? Actually, launchpad can create <strong>recipes</strong> above your repo. A recipe is just a debian packaging step, a bit like we did in the previous part. The power of launchpad is to automatize it and then host the builded packages.</p>
<p>Let us consider you are in your launchpad account. Go to the &ldquo;Code&rdquo; section. Here you may see nothing, in this case, click on &ldquo;View Git repositories&rdquo; (or directly <a href="https://code.launchpad.net/~USER/+git)">https://code.launchpad.net/~USER/+git)</a>. Then you will see your repo:</p>
<pre><code>Name 	                Status 	        Last Modified 	Last Commit
lp:~USER/+git/mytool 	Development 	... 	        ... 
</code></pre><p>You can click on it and then &ldquo;Create packaging recipe&rdquo;. You have to fill some basic information. A noticable thing is the <strong>Default distribution series</strong>: they are the Ubuntu versions for which the package will be built.</p>
<p>Moreover, the text of the recipe can be customized. In particular, you can change the debian versioning scheme. A common pattern is the following: <code>{debupstream}~{revno}</code> where <code>debupstream</code> is the classical version (i.e. <code>1.0</code> is our case) and <code>revno</code> is a counter incremented at each change.</p>
<p>Actually, all the parameters of the recipe can be changed afterwards.</p>
<p>Finally, on your recipe board (<a href="https://code.launchpad.net/~USER/+recipe/RECIPE_NAME">https://code.launchpad.net/~USER/+recipe/RECIPE_NAME</a>) you can request new builds. Once triggered, launchpad notifies the remaining time and after the status of the packaging step (success or fail) with logs.</p>
<p>If it fails, you have to check these logs and investigate errors.</p>
<h3 id="distribute">Distribute</h3>
<p>Once your builds succeed, you have available package in your PPA. For Ubuntu users, then can add your PPA to their <code>/etc/apt/sources.list</code>. Then they will be able to browse your package, install and update it easily.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo add-apt-repository ppa:USER/REPOSITORY-NAME
sudo apt update
sudo apt install mytool
</code></pre></div><h2 id="step-3-workflow">Step 3: Workflow</h2>
<p>Ok, we have a local git repo and two remote ones (GitHub+Launchpad for example). Builds are made on Launchpad (and are available through yout ppa), but your releases can also be hosted on GitHub.</p>
<h3 id="creating-a-new-version">Creating a new version</h3>
<p>You have your local source code and you make it directly available on your Github repo or through a debian package from your ppa.</p>
<p>Everytime you push to launchpad, it increments the revision number. So initially, launchpad builds <code>mytool_1.0-1</code> (with the scheme <code>{debupstream}~{revno}</code>) and then it will build <code>mytool_1.0-2</code>, <code>mytool_1.0-3</code> (at the second and third push) etc.</p>
<p>This is very nice because the user could receive these updates through its package manager.
However, you can make many minor commits (not deserving a new version) although sometimes you naturally commit some very huge and useful changes, putting your tool at higher level. At this moment, you want to make a new version!</p>
<p>Once again, we use <code>gbp</code>. In particular, the command <code>dch</code> generates Debian changelog entries from git commit messages. It means that all your commits (not registered in the previous version) will be written in the <code>debian/changelog</code> (this file also embeds the upstream version of your tool).</p>
<p>With the following command, you create a new version of your tool (1.1). It prompts you the changelog file on your favorite terminal text editor. So you can edit and verify all the details.</p>
<pre><code>gbp dch --new-version 1.1
</code></pre><p>If your work is quite stable you can add the <code>--release</code> option to mark it as a release.
You can also commit the changelog by adding the <code>--commit</code> option (the default message will be &ldquo;Update changelog for %(version)s release&rdquo;).</p>
<h3 id="working-with-github-releases">Working with GitHub releases</h3>
<p>You probably know that GitHub can manage releases through tags. When you create a new version, the idea would be to create the git tag in the same time so as to create a new package in the launchpad side and a new release on the GitHub side. And you can also upload the .deb packages created on launchpad to the GitHub release (as &ldquo;release assets&rdquo;).</p>
<p>The process is the following: add your new code to git, create the new version, commit, tag and push!</p>
<pre><code># add your changes
git add -u
# create the new version and commit everything
git dch --new-version X.X --commit
# tag the commit (its name will be &quot;debian/X.X&quot;)
gbp buildpackage --git-tag-only
# push the tag
git push origin --tags
git push launchpad --tags
</code></pre><p>Instead of using <code>gbp</code>, the tag can be done manually. The equivalent is:</p>
<pre><code>git tag -a debian/X.X -m &quot;Update changelog for X.X release&quot;
</code></pre><p>When you push the tag, you can also precise the tag you want to push (the command <code>--tags</code> push all the tags) through:</p>
<pre><code>git push origin debian/X.X
</code></pre><p>Warning: if you push normally on launchpad, a new package is naturally built. If after that you push the tag, some build/upload problems can occur because the revision number did not change (so it cannot replace the previous package with a new package with the same version).</p>
<h3 id="final-workflow">Final workflow</h3>
<pre><code># you have written new code ...
# you can check locally if your package builds correctly
gbp buildpackage

# if everything is ok, you can add your changes
git add -u

# if you want to create a new version...
# update the changelog (the --commit option will also commit the changes previously added)
git dch --new-version X.X --commit
# create the corresponding tag (its name will be &quot;debian/X.X&quot;)
gbp buildpackage --git-tag-only
# then push
git push origin --tags
git push launchpad --tags

# otherwise
git commit -m &quot;your minor changes&quot;
git push origin master
git push launchpad master
</code></pre>]]></content>
        </item>
        
    </channel>
</rss>
